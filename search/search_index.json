{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tremor RFCs - Active RFC List \u00b6 Many changes, including bug fixes and documentation improvements can be implemented and reviewed via the normal GitHub pull request workflow. Some changes though are \"substantial\", and we ask that these be put through a bit of a design process and produce a consensus among the Tremor community and the sub-teams. The \"RFC\" (request for comments) process is intended to provide a consistent and controlled path for new features to enter the project and associated sub projects and libraries, so that all stakeholders can be confident about the direction the project is evolving in. Table of Contents \u00b6 Opening Table of Contents When you need to follow this process Before creating an RFC What the process is The RFC life-cycle Reviewing RFCs Implementing an RFC RFC Postponement Help this is all too informal! License Origins When you need to follow this process \u00b6 You need to follow this process if you intend to make \"substantial\" changes to Tremor, Tremor-Script, Tremor-Query, Tremor pipelines, concurrency model, or the RFC process itself. What constitutes a \"substantial\" change is evolving based on community norms and varies depending on what part of the ecosystem you are proposing to change, but may include the following. Any semantic or syntactic change to the project that is not a bugfix. Removing features, including those that are feature-gated. Changes to the interface between the components, sub-systems and libraries, including tremor script function library and tremor query aggregate function library Some changes do not require an RFC: Rephrasing, reorganizing, refactoring, or otherwise \"changing shape does not change meaning\". Additions that strictly improve objective, numerical quality criteria (warning removal, speedup, better platform coverage, more parallelism, trap more errors, etc.) Additions only likely to be noticed by other developers-of-tremor, invisible to users-of-tremor. If you submit a pull request to implement a new feature without going through the RFC process, it may be closed with a polite request to submit an RFC first. Sub-team specific guidelines \u00b6 For more details on when an RFC is required for the following areas, please see the Tremor community's [sub-team] specific guidelines for: language changes , library changes , api changes . architecture changes . Before creating an RFC \u00b6 A hastily-proposed RFC can hurt its chances of acceptance. Low quality proposals, proposals for previously-rejected features, or those that don't fit into the near-term roadmap, may be quickly rejected, which can be demotivating for the unprepared contributor. Laying some groundwork ahead of the RFC can make the process smoother. Although there is no single way to prepare for submitting an RFC, it is generally a good idea to pursue feedback from other project developers beforehand, to ascertain that the RFC may be desirable; having a consistent impact on the project requires concerted effort toward consensus-building. The most common preparations for writing and submitting an RFC include talking the idea over on our tremor chat , and occasionally posting \"pre-RFCs\" on the chat environment. You may file issues on this repo for discussion, but these are not actively looked at by the teams. As a rule of thumb, receiving encouraging feedback from long-standing project developers, and particularly members of the relevant [sub-team] is a good indication that the RFC is worth pursuing. What the process is \u00b6 In short, to get a major feature added to Tremor, one must first get the RFC merged into the RFC repository as a markdown file. At that point the RFC is \"active\" and may be implemented with the goal of eventual inclusion into Tremor. Fork the RFC repo RFC repository Copy 0000-template.md to text/0000-my-feature.md (where \"my-feature\" is descriptive. don't assign an RFC number yet). Fill in the RFC. Put care into the details: RFCs that do not present convincing motivation, demonstrate lack of understanding of the design's impact, or are disingenuous about the drawbacks or alternatives tend to be poorly-received. Submit a pull request. As a pull request the RFC will receive design feedback from the larger community, and the author should be prepared to revise it in response. Each pull request will be labeled with the most relevant [sub-team], which will lead to its being triaged by that team in a future meeting and assigned to a member of the subteam. Build consensus and integrate feedback. RFCs that have broad support are much more likely to make progress than those that don't receive any comments. Feel free to reach out to the RFC assignee in particular to get help identifying stakeholders and obstacles. The sub-team will discuss the RFC pull request, as much as possible in the comment thread of the pull request itself. Offline discussion will be summarized on the pull request comment thread. RFCs rarely go through this process unchanged, especially as alternatives and drawbacks are shown. You can make edits, big and small, to the RFC to clarify or change the design, but make changes as new commits to the pull request, and leave a comment on the pull request explaining your changes. Specifically, do not squash or rebase commits after they are visible on the pull request. At some point, a member of the subteam will propose a \"motion for final comment period\" (FCP), along with a disposition for the RFC (merge, close, or postpone). This step is taken when enough of the tradeoffs have been discussed that the subteam is in a position to make a decision. That does not require consensus amongst all participants in the RFC thread (which is usually impossible). However, the argument supporting the disposition on the RFC needs to have already been clearly articulated, and there should not be a strong consensus against that position outside of the subteam. Subteam members use their best judgment in taking this step, and the FCP itself ensures there is ample time and notification for stakeholders to push back if it is made prematurely. For RFCs with lengthy discussion, the motion to FCP is usually preceded by a summary comment trying to lay out the current state of the discussion and major tradeoffs/points of disagreement. Before actually entering FCP, all members of the subteam must sign off; this is often the point at which many subteam members first review the RFC in full depth. The FCP lasts ten calendar days, so that it is open for at least 5 business days. In most cases, the FCP period is quiet, and the RFC is either merged or closed. However, sometimes substantial new arguments or ideas are raised, the FCP is canceled, and the RFC goes back into development mode. The RFC life-cycle \u00b6 Once an RFC becomes \"active\" then authors may implement it and submit the feature as a pull request to the Tremor repo. Being \"active\" is not a rubber stamp, and in particular still does not mean the feature will ultimately be merged; it does mean that in principle all the major stakeholders have agreed to the feature and are amenable to merging it. Furthermore, the fact that a given RFC has been accepted and is \"active\" implies nothing about what priority is assigned to its implementation, nor does it imply anything about whether a Tremor contributor has been assigned the task of implementing the task. While it is not necessary that the author of the RFC also pursue the implementation, it is by far the most effective way to see an RFC through to completion: authors should not expect that other project stakeholders will take on responsibility for implementing their accepted feature. Modifications to \"active\" RFCs can be done in follow-up pull requests. We strive to write each RFC in a manner that it will reflect the final design of the feature; but the nature of the process means that we cannot expect every merged RFC to actually reflect what the end result will be at the time of the next major release. In general, once accepted, RFCs should not be substantially changed. Only very minor changes should be submitted as amendments. More substantial changes should be new RFCs, with a note added to the original RFC. Exactly what counts as a \"very minor change\" is up to the sub-team to decide; check Sub-team specific guidelines for more details. Reviewing RFCs \u00b6 While the RFC pull request is up, the sub-team may schedule meetings with the author and/or relevant stakeholders to discuss the issues in greater detail, and in some cases the topic may be discussed at a sub-team meeting. In either case a summary from the meeting will be posted back to the RFC pull request. A sub-team makes final decisions about RFCs after the benefits and drawbacks are well understood. These decisions can be made at any time, but the sub-team will regularly issue decisions. When a decision is made, the RFC pull request will either be merged or closed. In either case, if the reasoning is not clear from the discussion in thread, the sub-team will add a comment describing the rationale for the decision. Implementing an RFC \u00b6 Some accepted RFCs represent vital features that need to be implemented right away. Other accepted RFCs can represent features that can wait until some arbitrary developer feels like doing the work. Every accepted RFC has an associated issue tracking its implementation in the Rust repository; thus that associated issue can be assigned a priority via the triage process that the team uses for all issues in the Rust repository. The author of an RFC is not obligated to implement it. Of course, the RFC author (like any other developer) is welcome to post an implementation for review after the RFC has been accepted. If you are interested in working on the implementation for an \"active\" RFC, but cannot determine if someone else is already working on it, feel free to ask (e.g. by leaving a comment on the associated issue). RFC Postponement \u00b6 Some RFC pull requests are tagged with the \"postponed\" label when they are closed (as part of the rejection process). An RFC closed with \"postponed\" is marked as such because we want neither to think about evaluating the proposal nor about implementing the described feature until some time in the future, and we believe that we can afford to wait until then to do so. Postponed pull requests may be re-opened when the time is right. We don't have any formal process for that, you should ask members of the relevant sub-team. Usually an RFC pull request marked as \"postponed\" has already passed an informal first round of evaluation, namely the round of \"do we think we would ever possibly consider making this change, as outlined in the RFC pull request, or some semi-obvious variation of it.\" (When the answer to the latter question is \"no\", then the appropriate response is to close the RFC, not postpone it.) Help this is all too informal! \u00b6 The process is intended to be as lightweight as reasonable for the present circumstances. As usual, we are trying to let the process be driven by consensus and community norms, not impose more structure than necessary. License \u00b6 This repository is currently licensed under: Apache License, Version 2.0, ( LICENSE or http://www.apache.org/licenses/LICENSE-2.0) Contributions \u00b6 Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions. Origins \u00b6 This process derives from and is based on the rust language community rfc process from here","title":"Home"},{"location":"#tremor-rfcs-active-rfc-list","text":"Many changes, including bug fixes and documentation improvements can be implemented and reviewed via the normal GitHub pull request workflow. Some changes though are \"substantial\", and we ask that these be put through a bit of a design process and produce a consensus among the Tremor community and the sub-teams. The \"RFC\" (request for comments) process is intended to provide a consistent and controlled path for new features to enter the project and associated sub projects and libraries, so that all stakeholders can be confident about the direction the project is evolving in.","title":"Tremor RFCs - Active RFC List"},{"location":"#table-of-contents","text":"Opening Table of Contents When you need to follow this process Before creating an RFC What the process is The RFC life-cycle Reviewing RFCs Implementing an RFC RFC Postponement Help this is all too informal! License Origins","title":"Table of Contents"},{"location":"#when-you-need-to-follow-this-process","text":"You need to follow this process if you intend to make \"substantial\" changes to Tremor, Tremor-Script, Tremor-Query, Tremor pipelines, concurrency model, or the RFC process itself. What constitutes a \"substantial\" change is evolving based on community norms and varies depending on what part of the ecosystem you are proposing to change, but may include the following. Any semantic or syntactic change to the project that is not a bugfix. Removing features, including those that are feature-gated. Changes to the interface between the components, sub-systems and libraries, including tremor script function library and tremor query aggregate function library Some changes do not require an RFC: Rephrasing, reorganizing, refactoring, or otherwise \"changing shape does not change meaning\". Additions that strictly improve objective, numerical quality criteria (warning removal, speedup, better platform coverage, more parallelism, trap more errors, etc.) Additions only likely to be noticed by other developers-of-tremor, invisible to users-of-tremor. If you submit a pull request to implement a new feature without going through the RFC process, it may be closed with a polite request to submit an RFC first.","title":"When you need to follow this process"},{"location":"#sub-team-specific-guidelines","text":"For more details on when an RFC is required for the following areas, please see the Tremor community's [sub-team] specific guidelines for: language changes , library changes , api changes . architecture changes .","title":"Sub-team specific guidelines"},{"location":"#before-creating-an-rfc","text":"A hastily-proposed RFC can hurt its chances of acceptance. Low quality proposals, proposals for previously-rejected features, or those that don't fit into the near-term roadmap, may be quickly rejected, which can be demotivating for the unprepared contributor. Laying some groundwork ahead of the RFC can make the process smoother. Although there is no single way to prepare for submitting an RFC, it is generally a good idea to pursue feedback from other project developers beforehand, to ascertain that the RFC may be desirable; having a consistent impact on the project requires concerted effort toward consensus-building. The most common preparations for writing and submitting an RFC include talking the idea over on our tremor chat , and occasionally posting \"pre-RFCs\" on the chat environment. You may file issues on this repo for discussion, but these are not actively looked at by the teams. As a rule of thumb, receiving encouraging feedback from long-standing project developers, and particularly members of the relevant [sub-team] is a good indication that the RFC is worth pursuing.","title":"Before creating an RFC"},{"location":"#what-the-process-is","text":"In short, to get a major feature added to Tremor, one must first get the RFC merged into the RFC repository as a markdown file. At that point the RFC is \"active\" and may be implemented with the goal of eventual inclusion into Tremor. Fork the RFC repo RFC repository Copy 0000-template.md to text/0000-my-feature.md (where \"my-feature\" is descriptive. don't assign an RFC number yet). Fill in the RFC. Put care into the details: RFCs that do not present convincing motivation, demonstrate lack of understanding of the design's impact, or are disingenuous about the drawbacks or alternatives tend to be poorly-received. Submit a pull request. As a pull request the RFC will receive design feedback from the larger community, and the author should be prepared to revise it in response. Each pull request will be labeled with the most relevant [sub-team], which will lead to its being triaged by that team in a future meeting and assigned to a member of the subteam. Build consensus and integrate feedback. RFCs that have broad support are much more likely to make progress than those that don't receive any comments. Feel free to reach out to the RFC assignee in particular to get help identifying stakeholders and obstacles. The sub-team will discuss the RFC pull request, as much as possible in the comment thread of the pull request itself. Offline discussion will be summarized on the pull request comment thread. RFCs rarely go through this process unchanged, especially as alternatives and drawbacks are shown. You can make edits, big and small, to the RFC to clarify or change the design, but make changes as new commits to the pull request, and leave a comment on the pull request explaining your changes. Specifically, do not squash or rebase commits after they are visible on the pull request. At some point, a member of the subteam will propose a \"motion for final comment period\" (FCP), along with a disposition for the RFC (merge, close, or postpone). This step is taken when enough of the tradeoffs have been discussed that the subteam is in a position to make a decision. That does not require consensus amongst all participants in the RFC thread (which is usually impossible). However, the argument supporting the disposition on the RFC needs to have already been clearly articulated, and there should not be a strong consensus against that position outside of the subteam. Subteam members use their best judgment in taking this step, and the FCP itself ensures there is ample time and notification for stakeholders to push back if it is made prematurely. For RFCs with lengthy discussion, the motion to FCP is usually preceded by a summary comment trying to lay out the current state of the discussion and major tradeoffs/points of disagreement. Before actually entering FCP, all members of the subteam must sign off; this is often the point at which many subteam members first review the RFC in full depth. The FCP lasts ten calendar days, so that it is open for at least 5 business days. In most cases, the FCP period is quiet, and the RFC is either merged or closed. However, sometimes substantial new arguments or ideas are raised, the FCP is canceled, and the RFC goes back into development mode.","title":"What the process is"},{"location":"#the-rfc-life-cycle","text":"Once an RFC becomes \"active\" then authors may implement it and submit the feature as a pull request to the Tremor repo. Being \"active\" is not a rubber stamp, and in particular still does not mean the feature will ultimately be merged; it does mean that in principle all the major stakeholders have agreed to the feature and are amenable to merging it. Furthermore, the fact that a given RFC has been accepted and is \"active\" implies nothing about what priority is assigned to its implementation, nor does it imply anything about whether a Tremor contributor has been assigned the task of implementing the task. While it is not necessary that the author of the RFC also pursue the implementation, it is by far the most effective way to see an RFC through to completion: authors should not expect that other project stakeholders will take on responsibility for implementing their accepted feature. Modifications to \"active\" RFCs can be done in follow-up pull requests. We strive to write each RFC in a manner that it will reflect the final design of the feature; but the nature of the process means that we cannot expect every merged RFC to actually reflect what the end result will be at the time of the next major release. In general, once accepted, RFCs should not be substantially changed. Only very minor changes should be submitted as amendments. More substantial changes should be new RFCs, with a note added to the original RFC. Exactly what counts as a \"very minor change\" is up to the sub-team to decide; check Sub-team specific guidelines for more details.","title":"The RFC life-cycle"},{"location":"#reviewing-rfcs","text":"While the RFC pull request is up, the sub-team may schedule meetings with the author and/or relevant stakeholders to discuss the issues in greater detail, and in some cases the topic may be discussed at a sub-team meeting. In either case a summary from the meeting will be posted back to the RFC pull request. A sub-team makes final decisions about RFCs after the benefits and drawbacks are well understood. These decisions can be made at any time, but the sub-team will regularly issue decisions. When a decision is made, the RFC pull request will either be merged or closed. In either case, if the reasoning is not clear from the discussion in thread, the sub-team will add a comment describing the rationale for the decision.","title":"Reviewing RFCs"},{"location":"#implementing-an-rfc","text":"Some accepted RFCs represent vital features that need to be implemented right away. Other accepted RFCs can represent features that can wait until some arbitrary developer feels like doing the work. Every accepted RFC has an associated issue tracking its implementation in the Rust repository; thus that associated issue can be assigned a priority via the triage process that the team uses for all issues in the Rust repository. The author of an RFC is not obligated to implement it. Of course, the RFC author (like any other developer) is welcome to post an implementation for review after the RFC has been accepted. If you are interested in working on the implementation for an \"active\" RFC, but cannot determine if someone else is already working on it, feel free to ask (e.g. by leaving a comment on the associated issue).","title":"Implementing an RFC"},{"location":"#rfc-postponement","text":"Some RFC pull requests are tagged with the \"postponed\" label when they are closed (as part of the rejection process). An RFC closed with \"postponed\" is marked as such because we want neither to think about evaluating the proposal nor about implementing the described feature until some time in the future, and we believe that we can afford to wait until then to do so. Postponed pull requests may be re-opened when the time is right. We don't have any formal process for that, you should ask members of the relevant sub-team. Usually an RFC pull request marked as \"postponed\" has already passed an informal first round of evaluation, namely the round of \"do we think we would ever possibly consider making this change, as outlined in the RFC pull request, or some semi-obvious variation of it.\" (When the answer to the latter question is \"no\", then the appropriate response is to close the RFC, not postpone it.)","title":"RFC Postponement"},{"location":"#help-this-is-all-too-informal","text":"The process is intended to be as lightweight as reasonable for the present circumstances. As usual, we are trying to let the process be driven by consensus and community norms, not impose more structure than necessary.","title":"Help this is all too informal!"},{"location":"#license","text":"This repository is currently licensed under: Apache License, Version 2.0, ( LICENSE or http://www.apache.org/licenses/LICENSE-2.0)","title":"License"},{"location":"#contributions","text":"Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions.","title":"Contributions"},{"location":"#origins","text":"This process derives from and is based on the rust language community rfc process from here","title":"Origins"},{"location":"0000-template/","text":"Feature Name: (fill me in with a unique ident, my_awesome_feature ) Start Date: (fill me in with today's date, YYYY-MM-DD) Tremor Issue: tremor-rs/tremor-runtime#0000 RFC PR: tremor-rs/tremor-rfcs#0000 Summary \u00b6 One paragraph explanation of the feature. Motivation \u00b6 Why are we doing this? What use cases does it support? What is the expected outcome? Guide-level explanation \u00b6 Explain the proposal as if it was already included in Tremor and you were teaching it to another Tremor user. That generally means: Introducing new named concepts. Explaining the feature largely in terms of examples. Explaining how stakeholders should think about the feature, and how it should impact the way they use tremor. It should explain the impact as concretely as possible. If applicable, provide sample error messages, deprecation warnings, or migration guidance. If applicable, describe the differences between teaching this to existing tremor stakeholders and new tremor programmers. For implementation-oriented RFCs (e.g. for language internals), this section should focus on how language contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms. Reference-level explanation \u00b6 This is the technical portion of the RFC. Explain the design in sufficient detail that: Its interaction with other features is clear. It is reasonably clear how the feature would be implemented. Corner cases are dissected by example. The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work. Drawbacks \u00b6 Why should we not do this? Rationale and alternatives \u00b6 Why is this design the best in the space of possible designs? What other designs have been considered and what is the rationale for not choosing them? What is the impact of not doing this? Prior art \u00b6 Discuss prior art, both the good and the bad, in relation to this proposal. A few examples of what this can include are: For language, library, tools, and clustering proposals: Does this feature exist in other programming languages and what experience have their community had? For community proposals: Is this done by some other community and what were their experiences with it? For other teams: What lessons can we learn from what other communities have done here? Papers: Are there any published papers or great posts that discuss this? If you have some relevant papers to refer to, this can serve as a more detailed theoretical background. This section is intended to encourage you as an author to think about the lessons from other projects, provide readers of your RFC with a fuller picture. If there is no prior art, that is fine - your ideas are interesting to us whether they are brand new or if it is an adaptation from other projects. Note that while precedent set by other projects is some motivation, it does not on its own motivate an RFC. Please also take into consideration that tremor sometimes intentionally diverges from similar projects. Unresolved questions \u00b6 What parts of the design do you expect to resolve through the RFC process before this gets merged? What parts of the design do you expect to resolve through the implementation of this feature before stabilization? What related issues do you consider out of scope for this RFC that could be addressed in the future independently of the solution that comes out of this RFC? Future possibilities \u00b6 Think about what the natural extension and evolution of your proposal would be and how it would affect Tremor as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project in your proposal. Also consider how the this all fits into the roadmap for the project and of the relevant sub-team. This is also a good place to \"dump ideas\", if they are out of scope for the RFC you are writing but otherwise related. If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything. Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC; such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.","title":"RFC Template"},{"location":"0000-template/#summary","text":"One paragraph explanation of the feature.","title":"Summary"},{"location":"0000-template/#motivation","text":"Why are we doing this? What use cases does it support? What is the expected outcome?","title":"Motivation"},{"location":"0000-template/#guide-level-explanation","text":"Explain the proposal as if it was already included in Tremor and you were teaching it to another Tremor user. That generally means: Introducing new named concepts. Explaining the feature largely in terms of examples. Explaining how stakeholders should think about the feature, and how it should impact the way they use tremor. It should explain the impact as concretely as possible. If applicable, provide sample error messages, deprecation warnings, or migration guidance. If applicable, describe the differences between teaching this to existing tremor stakeholders and new tremor programmers. For implementation-oriented RFCs (e.g. for language internals), this section should focus on how language contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.","title":"Guide-level explanation"},{"location":"0000-template/#reference-level-explanation","text":"This is the technical portion of the RFC. Explain the design in sufficient detail that: Its interaction with other features is clear. It is reasonably clear how the feature would be implemented. Corner cases are dissected by example. The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.","title":"Reference-level explanation"},{"location":"0000-template/#drawbacks","text":"Why should we not do this?","title":"Drawbacks"},{"location":"0000-template/#rationale-and-alternatives","text":"Why is this design the best in the space of possible designs? What other designs have been considered and what is the rationale for not choosing them? What is the impact of not doing this?","title":"Rationale and alternatives"},{"location":"0000-template/#prior-art","text":"Discuss prior art, both the good and the bad, in relation to this proposal. A few examples of what this can include are: For language, library, tools, and clustering proposals: Does this feature exist in other programming languages and what experience have their community had? For community proposals: Is this done by some other community and what were their experiences with it? For other teams: What lessons can we learn from what other communities have done here? Papers: Are there any published papers or great posts that discuss this? If you have some relevant papers to refer to, this can serve as a more detailed theoretical background. This section is intended to encourage you as an author to think about the lessons from other projects, provide readers of your RFC with a fuller picture. If there is no prior art, that is fine - your ideas are interesting to us whether they are brand new or if it is an adaptation from other projects. Note that while precedent set by other projects is some motivation, it does not on its own motivate an RFC. Please also take into consideration that tremor sometimes intentionally diverges from similar projects.","title":"Prior art"},{"location":"0000-template/#unresolved-questions","text":"What parts of the design do you expect to resolve through the RFC process before this gets merged? What parts of the design do you expect to resolve through the implementation of this feature before stabilization? What related issues do you consider out of scope for this RFC that could be addressed in the future independently of the solution that comes out of this RFC?","title":"Unresolved questions"},{"location":"0000-template/#future-possibilities","text":"Think about what the natural extension and evolution of your proposal would be and how it would affect Tremor as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project in your proposal. Also consider how the this all fits into the roadmap for the project and of the relevant sub-team. This is also a good place to \"dump ideas\", if they are out of scope for the RFC you are writing but otherwise related. If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything. Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC; such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.","title":"Future possibilities"},{"location":"0001-remove-actix-from-tremor-runtime/","text":"Feature Name: rfc_0001_remove_actix Start Date: 2020-01-20 Issue: tremor-rs/tremor-rfcs#0001 RFC PR: tremor-rs/tremor-rfcs#0002 Summary \u00b6 Actix should be replaced with a suitable alternative. Motivation \u00b6 The actix project in its current form has been discontinued. Actix is used within the tremor-runtime for control plane facilities off the critical path, such as hosting the tremor API and is used in some onramps/offramps. A suitable replacement should be integrated replacing actix. Guide-level explanation \u00b6 The Tremor REST API uses the actix-web project. The Tremor server uses actix, actix-files actix-cors and actix-web. These projects are discontinued and need suitable replacements. Reference-level explanation \u00b6 REST API use case: Adopt (tide)[https://github.com/http-rs/tide] for REST and HTTP API purposes WebSocket API use csae: Adopt (async-tungstenite)[https://crates.io/crates/async-tungstenite] Control plane use case: Prefer interceptor interface insulating control plane from transport/data format choices Replace all uses of core actix actors with a suitable alternative Consider channels with event/run loops as an alternative pseudo-actor model Consider (bastion)[https://docs.rs/bastion] Drawbacks \u00b6 Tremor-runtime is a working system and is currently stable. Replacing the control plane, API and websocket facilities will incur some overhead and risk especially regarding differences in actor model implementations and how they interoperate with asynchronous or channel based rust code which tremor exploits heavily. Rationale and alternatives \u00b6 The existing test suites in EQC should be sufficient for API purposes to defend against regression but no alternative is stable, and there is no guarantee that current external frameworks will be supported in the longer term. Actix may yet find alternative maintainers and/or it could be forked and maintained by ourselves, worst case to preserve current investment in the actix project within tremor. Actix has an opinionated architecture and implementation which deviates from the goals and needs of tremor sufficiently that its integration was not without compromise. Revisiting areas of tremor code currently adopting actix may result in simplification and modernising to current Rust style and idioms based on fully standards-based async rust, which was not available when these facilities were implemented. Prior art \u00b6 As above. Unresolved questions \u00b6 Consider maintenance dynamics of libraries being considered for adoption with a preferential biases for smaller libraries that would be easier to maintain if their maintainership changes looking forward. Future possibilities \u00b6 Generalise on async for all facilities currently integrating actix and reconsider tremor runtime, concurrency and other primitives with async rust as an optimising bias.","title":"0001-remove-actix-from-tremor-runtime"},{"location":"0001-remove-actix-from-tremor-runtime/#summary","text":"Actix should be replaced with a suitable alternative.","title":"Summary"},{"location":"0001-remove-actix-from-tremor-runtime/#motivation","text":"The actix project in its current form has been discontinued. Actix is used within the tremor-runtime for control plane facilities off the critical path, such as hosting the tremor API and is used in some onramps/offramps. A suitable replacement should be integrated replacing actix.","title":"Motivation"},{"location":"0001-remove-actix-from-tremor-runtime/#guide-level-explanation","text":"The Tremor REST API uses the actix-web project. The Tremor server uses actix, actix-files actix-cors and actix-web. These projects are discontinued and need suitable replacements.","title":"Guide-level explanation"},{"location":"0001-remove-actix-from-tremor-runtime/#reference-level-explanation","text":"REST API use case: Adopt (tide)[https://github.com/http-rs/tide] for REST and HTTP API purposes WebSocket API use csae: Adopt (async-tungstenite)[https://crates.io/crates/async-tungstenite] Control plane use case: Prefer interceptor interface insulating control plane from transport/data format choices Replace all uses of core actix actors with a suitable alternative Consider channels with event/run loops as an alternative pseudo-actor model Consider (bastion)[https://docs.rs/bastion]","title":"Reference-level explanation"},{"location":"0001-remove-actix-from-tremor-runtime/#drawbacks","text":"Tremor-runtime is a working system and is currently stable. Replacing the control plane, API and websocket facilities will incur some overhead and risk especially regarding differences in actor model implementations and how they interoperate with asynchronous or channel based rust code which tremor exploits heavily.","title":"Drawbacks"},{"location":"0001-remove-actix-from-tremor-runtime/#rationale-and-alternatives","text":"The existing test suites in EQC should be sufficient for API purposes to defend against regression but no alternative is stable, and there is no guarantee that current external frameworks will be supported in the longer term. Actix may yet find alternative maintainers and/or it could be forked and maintained by ourselves, worst case to preserve current investment in the actix project within tremor. Actix has an opinionated architecture and implementation which deviates from the goals and needs of tremor sufficiently that its integration was not without compromise. Revisiting areas of tremor code currently adopting actix may result in simplification and modernising to current Rust style and idioms based on fully standards-based async rust, which was not available when these facilities were implemented.","title":"Rationale and alternatives"},{"location":"0001-remove-actix-from-tremor-runtime/#prior-art","text":"As above.","title":"Prior art"},{"location":"0001-remove-actix-from-tremor-runtime/#unresolved-questions","text":"Consider maintenance dynamics of libraries being considered for adoption with a preferential biases for smaller libraries that would be easier to maintain if their maintainership changes looking forward.","title":"Unresolved questions"},{"location":"0001-remove-actix-from-tremor-runtime/#future-possibilities","text":"Generalise on async for all facilities currently integrating actix and reconsider tremor runtime, concurrency and other primitives with async rust as an optimising bias.","title":"Future possibilities"},{"location":"0002-pipeline-state-mechanism/","text":"Feature Name: rfc_0002_pipeline_state_mechanism Start Date: 2020-01-22 Issue: tremor-rs/tremor-rfcs#0003 RFC PR: tremor-rs/tremor-rfcs#0004 Summary \u00b6 Legacy tremor YAML configured tremor pipeline and Trickle query language pipelines currently do not track state across events over time. A mechanism is required to introduce state management and storage facilities to the tremor runtime and made available to pipeline implementations. Motivation \u00b6 The absence of a state mechanism limits the usefulness and extent of algorithms that can be implemented by tremor to those that are stateless, or those that leverage builtin custom operators that maintain state such as the 'bucket' or 'batch' operators. A state mechanism and supporting user-facing facilities would allow users to exploit stateful algorithms for session tracking, building and maintaining application state or for the query language to evolve support for in memory or persistent tables. Guide-level explanation \u00b6 The state mechanism in tremor pipelines allows operator node-level state management and storage that persists for the running lifetime of a pipeline algorithm deployed into the tremor runtime. The state mechanism introduces the state keyword into the tremor scripting language. This new keyword provides access to the state storage contents via path expressions (much like how the event keyword works, with the key difference being that the state storage is shared across events). On pipeline initialization, the state will be initialized as null and users are free to set it to arbitrary value over the course of processing. Here's a tremor-script example demonstrating the usage of state mechanism -- it maintains a counter for the events coming in and emits the count alongside the event: match type :: is_null ( state ) of case true => let state = { \"count\" : 1 } default => let state . count = state . count + 1 end ; { \"count\" : state . count , \"event\" : event } This will work as part of the runtime::tremor operator confguration in the legacy pipeline yaml setup, and also as an embedded script in the trickle definition of the pipeline. Other pipeline operators can utilize the same underlying state storage. An example is the new generic::counter operator that replicates the functionality above: define generic :: counter operator my_counter ; create operator my_counter ; select event from in into my_counter ; select event from my_counter into out ; State is not shared across operator nodes i.e. we have separate state storage for each operator instance and an operator can access only the storage associated with the operator. Reference-level explanation \u00b6 A new struct to encapsulate state across each of the operator nodes should be introduced. Example: struct State { // this vector holds the state value for each operator node ops : Vec < Value <' static >> , } Inside the struct, the operator node-level state can then be ordered in the same way as nodes in the pipeline graph, following the strategy we have in use already for storing the node-level metrics in the graph. When the event is passed to the operator node for processing, state specific to the operator can then be passed on by keying on the index of the node in the graph. This ensures that state is not shared across the operator instances. The State struct will be initialialized on pipeline creation, and will be destroyed on destruction of a pipline when it is undeployed or the host process is shut down. Effectively the state mechanism encapsulates the entire micro-state of a pipeline and any captured user defined logic in a supported scripting language or operator in a pipeline. This allows pipeline state to be recorded in a snapshot to support advanced use cases such as pipeline migration through coordinated passivation, serialization, migration, deserialization and re-activation of a pipeline on a different tremor-runtime node without loss of state. In the tremor scripting / query language the state keyword provides a reference onto the associated operator specifc state managed by the runtime. Drawbacks \u00b6 Tremor-runtime is a working system and is currently stable. Since the implementation of the state mechanism will touch the main event pathways throughout the pipeline as well as across the scripting language, it has the potential to introduce inefficiencies as well as instability, if not done right. By consolidating on a single namespace state we remain consistent with other specialized keyword forms such as args , group , window that have special meaning in tremor in different contexts/situations. This introduces cognitive dissonance to the user (but in a managed way). Regarding the generic::counter operator that we have proposed to introduce as a demonstration of the state mechanism usage in a custom in-built operator, it may be that it won't find actual real-world usage and it will continue to languish in our codebase just as an example. Even for the case when people need such a functionality, it is trivially replicated from tremor-script (such an example was presented above too). Rationale and alternatives \u00b6 In this RFC, the basic mechanism as outlined can be implemented and exposed to the user with fairly minimal changes to the script / query language required to support an implementation. An alternative leveraging the metadata facility and usurping the $state namespace would result in marginally less implementation effort, but risks opening up other constraints to the metadata namespace. Such changes are user-impacting and as such not desired. Prior art \u00b6 None. Unresolved questions \u00b6 This RFC does not specify full internals or implementation of the state mechanism as it applies to operators. It is assumed that a state variable will be available to event handlers by the runtime that are managed by the runtime and partitioned by operator. This RFC limits the state that an operator can have to the Value type, which may not be suitable for all our operator state needs. An example is the LRU cache currently in use by the bucketing operator, or the desire to seed and cache a random number generator for use in tremor-script's random module functions. For such needs, we can continue to implement stateful implementations outside of the pipeline state mechanism and for the cases when we do need to store them somewhere central (eg: to enable pipeline migrations as part of clustering effort), we can opt to serialize these data structures into (and deserialize out) of the pipeline state. We will revisit this topic in the future when such needs arise. Future possibilities \u00b6 This RFC normatively reserves the state keyword for pipeline state management. The internal structure (schema) of the implied state struct is managed by this RFC. This RFC should be updated if the internal structure (schema) of the implied state record is further specified in the future (eg: we add an attribute to the struct to support state global to the pipeline). Other operators that maintain state can be migrated to use the new pipeline state mechanism (eg: for the batch or backpressure operator) -- this would be a necissity when we want to support pipeline migration to a different tremor node, when we have clustering for tremor.","title":"0002-pipeline-state-mechanism"},{"location":"0002-pipeline-state-mechanism/#summary","text":"Legacy tremor YAML configured tremor pipeline and Trickle query language pipelines currently do not track state across events over time. A mechanism is required to introduce state management and storage facilities to the tremor runtime and made available to pipeline implementations.","title":"Summary"},{"location":"0002-pipeline-state-mechanism/#motivation","text":"The absence of a state mechanism limits the usefulness and extent of algorithms that can be implemented by tremor to those that are stateless, or those that leverage builtin custom operators that maintain state such as the 'bucket' or 'batch' operators. A state mechanism and supporting user-facing facilities would allow users to exploit stateful algorithms for session tracking, building and maintaining application state or for the query language to evolve support for in memory or persistent tables.","title":"Motivation"},{"location":"0002-pipeline-state-mechanism/#guide-level-explanation","text":"The state mechanism in tremor pipelines allows operator node-level state management and storage that persists for the running lifetime of a pipeline algorithm deployed into the tremor runtime. The state mechanism introduces the state keyword into the tremor scripting language. This new keyword provides access to the state storage contents via path expressions (much like how the event keyword works, with the key difference being that the state storage is shared across events). On pipeline initialization, the state will be initialized as null and users are free to set it to arbitrary value over the course of processing. Here's a tremor-script example demonstrating the usage of state mechanism -- it maintains a counter for the events coming in and emits the count alongside the event: match type :: is_null ( state ) of case true => let state = { \"count\" : 1 } default => let state . count = state . count + 1 end ; { \"count\" : state . count , \"event\" : event } This will work as part of the runtime::tremor operator confguration in the legacy pipeline yaml setup, and also as an embedded script in the trickle definition of the pipeline. Other pipeline operators can utilize the same underlying state storage. An example is the new generic::counter operator that replicates the functionality above: define generic :: counter operator my_counter ; create operator my_counter ; select event from in into my_counter ; select event from my_counter into out ; State is not shared across operator nodes i.e. we have separate state storage for each operator instance and an operator can access only the storage associated with the operator.","title":"Guide-level explanation"},{"location":"0002-pipeline-state-mechanism/#reference-level-explanation","text":"A new struct to encapsulate state across each of the operator nodes should be introduced. Example: struct State { // this vector holds the state value for each operator node ops : Vec < Value <' static >> , } Inside the struct, the operator node-level state can then be ordered in the same way as nodes in the pipeline graph, following the strategy we have in use already for storing the node-level metrics in the graph. When the event is passed to the operator node for processing, state specific to the operator can then be passed on by keying on the index of the node in the graph. This ensures that state is not shared across the operator instances. The State struct will be initialialized on pipeline creation, and will be destroyed on destruction of a pipline when it is undeployed or the host process is shut down. Effectively the state mechanism encapsulates the entire micro-state of a pipeline and any captured user defined logic in a supported scripting language or operator in a pipeline. This allows pipeline state to be recorded in a snapshot to support advanced use cases such as pipeline migration through coordinated passivation, serialization, migration, deserialization and re-activation of a pipeline on a different tremor-runtime node without loss of state. In the tremor scripting / query language the state keyword provides a reference onto the associated operator specifc state managed by the runtime.","title":"Reference-level explanation"},{"location":"0002-pipeline-state-mechanism/#drawbacks","text":"Tremor-runtime is a working system and is currently stable. Since the implementation of the state mechanism will touch the main event pathways throughout the pipeline as well as across the scripting language, it has the potential to introduce inefficiencies as well as instability, if not done right. By consolidating on a single namespace state we remain consistent with other specialized keyword forms such as args , group , window that have special meaning in tremor in different contexts/situations. This introduces cognitive dissonance to the user (but in a managed way). Regarding the generic::counter operator that we have proposed to introduce as a demonstration of the state mechanism usage in a custom in-built operator, it may be that it won't find actual real-world usage and it will continue to languish in our codebase just as an example. Even for the case when people need such a functionality, it is trivially replicated from tremor-script (such an example was presented above too).","title":"Drawbacks"},{"location":"0002-pipeline-state-mechanism/#rationale-and-alternatives","text":"In this RFC, the basic mechanism as outlined can be implemented and exposed to the user with fairly minimal changes to the script / query language required to support an implementation. An alternative leveraging the metadata facility and usurping the $state namespace would result in marginally less implementation effort, but risks opening up other constraints to the metadata namespace. Such changes are user-impacting and as such not desired.","title":"Rationale and alternatives"},{"location":"0002-pipeline-state-mechanism/#prior-art","text":"None.","title":"Prior art"},{"location":"0002-pipeline-state-mechanism/#unresolved-questions","text":"This RFC does not specify full internals or implementation of the state mechanism as it applies to operators. It is assumed that a state variable will be available to event handlers by the runtime that are managed by the runtime and partitioned by operator. This RFC limits the state that an operator can have to the Value type, which may not be suitable for all our operator state needs. An example is the LRU cache currently in use by the bucketing operator, or the desire to seed and cache a random number generator for use in tremor-script's random module functions. For such needs, we can continue to implement stateful implementations outside of the pipeline state mechanism and for the cases when we do need to store them somewhere central (eg: to enable pipeline migrations as part of clustering effort), we can opt to serialize these data structures into (and deserialize out) of the pipeline state. We will revisit this topic in the future when such needs arise.","title":"Unresolved questions"},{"location":"0002-pipeline-state-mechanism/#future-possibilities","text":"This RFC normatively reserves the state keyword for pipeline state management. The internal structure (schema) of the implied state struct is managed by this RFC. This RFC should be updated if the internal structure (schema) of the implied state record is further specified in the future (eg: we add an attribute to the struct to support state global to the pipeline). Other operators that maintain state can be migrated to use the new pipeline state mechanism (eg: for the batch or backpressure operator) -- this would be a necissity when we want to support pipeline migration to a different tremor node, when we have clustering for tremor.","title":"Future possibilities"},{"location":"0003-linked-transports/","text":"Feature Name: rfc_0003_linked_transports Start Date: 2020-01-27 Issue: tremor-rs/tremor-rfcs#0005 RFC PR: tremor-rs/tremor-rfcs#0006 Summary \u00b6 The tremor-runtime supports ingress from and egress to external data sources and sinks through adapters called Onramps and Offramps accordingly. There is no mechanism currently that allows the underlying transport encapsulated by Onramps or Offramps to share their respective underlying connections. This RFC addresses these limitations by introducing linked transports notionally and allowing Onramp and Offramp specifications to reference a common shared underlying transport instance by reference. Motivation \u00b6 The absence of linked transports prohibits authors of tremor-script / tremor-query from writing proxy applications where request/response style interactions can be routed from a client request back to a client response in the same synchronous blocking RPC transport connection context back to the originating ephemeral client. This limitation also requires having multiple messaging endpoint connections for asynchronous communications when a single connection may be sufficient. Guide-level explanation \u00b6 Given a linked transport configuration: transport : - id : rest codec : json config : port : 8080 listen : - 0.0.0.0 headers : 'Content-type' : 'application/json' methods : '/api/{id}' : [ 'GET' , 'POST' , 'PUT' , 'PATCH' , 'DELETE' ] An onramp referencing the transport: onramp : - id : rest_inbound ref : rest An offramp referencing the same transport: offramp : - id : rest_outbound ref : rest Then, user defined logic in the script or query language can fully proxy or implement services for the underlying transport: match event of case % { path == \"/api/{id}\" , method == \"GET\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"POST\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 201 } , case % { path == \"/api/{id}\" , method == \"PUT\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"PATCH\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"DELETE\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 204 } , default => { \"error\" : \"Service unavailable\" , \"status\" : 503 } end The binding specification: binding : - id : api_gateway links : '/onramp/rest_inbound/{instance}/out' : [ '/pipeline/api/{instance}/in' ] '/pipeline/api/{instance}/out' : [ '/offramp/rest_outbound/{instance}/in' ] There is no facility in the current tremor-runtime to describe transport services that effectively implement or proxy an underlying protocol fully as there is no mechanism to intercept a request, process it through a pipeline, and route the response such that it consummates a single transport level request/response or messaging interaction. A shared transport fills this gap and allows the tremor-runtime to implement API gateways, to act as a HTTP router, proxy, reverse proxy and to implement similar capabilities for other transports. Reference-level explanation \u00b6 None. Drawbacks \u00b6 None. Rationale and alternatives \u00b6 The introduction of transports coupled with adding optional transport references to onramp and offramp specifications enables the tremor-runtime to act as a proxy or reverse proxy endpoint with logic implemented in the scripting or query language. Prior art \u00b6 None. Unresolved questions \u00b6 This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation. Future possibilities \u00b6 None known at this time.","title":"0003-linked-transports"},{"location":"0003-linked-transports/#summary","text":"The tremor-runtime supports ingress from and egress to external data sources and sinks through adapters called Onramps and Offramps accordingly. There is no mechanism currently that allows the underlying transport encapsulated by Onramps or Offramps to share their respective underlying connections. This RFC addresses these limitations by introducing linked transports notionally and allowing Onramp and Offramp specifications to reference a common shared underlying transport instance by reference.","title":"Summary"},{"location":"0003-linked-transports/#motivation","text":"The absence of linked transports prohibits authors of tremor-script / tremor-query from writing proxy applications where request/response style interactions can be routed from a client request back to a client response in the same synchronous blocking RPC transport connection context back to the originating ephemeral client. This limitation also requires having multiple messaging endpoint connections for asynchronous communications when a single connection may be sufficient.","title":"Motivation"},{"location":"0003-linked-transports/#guide-level-explanation","text":"Given a linked transport configuration: transport : - id : rest codec : json config : port : 8080 listen : - 0.0.0.0 headers : 'Content-type' : 'application/json' methods : '/api/{id}' : [ 'GET' , 'POST' , 'PUT' , 'PATCH' , 'DELETE' ] An onramp referencing the transport: onramp : - id : rest_inbound ref : rest An offramp referencing the same transport: offramp : - id : rest_outbound ref : rest Then, user defined logic in the script or query language can fully proxy or implement services for the underlying transport: match event of case % { path == \"/api/{id}\" , method == \"GET\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"POST\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 201 } , case % { path == \"/api/{id}\" , method == \"PUT\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"PATCH\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 200 } , case % { path == \"/api/{id}\" , method == \"DELETE\" } => { \"id\" : \"{event.params.id}\" , \"status\" : 204 } , default => { \"error\" : \"Service unavailable\" , \"status\" : 503 } end The binding specification: binding : - id : api_gateway links : '/onramp/rest_inbound/{instance}/out' : [ '/pipeline/api/{instance}/in' ] '/pipeline/api/{instance}/out' : [ '/offramp/rest_outbound/{instance}/in' ] There is no facility in the current tremor-runtime to describe transport services that effectively implement or proxy an underlying protocol fully as there is no mechanism to intercept a request, process it through a pipeline, and route the response such that it consummates a single transport level request/response or messaging interaction. A shared transport fills this gap and allows the tremor-runtime to implement API gateways, to act as a HTTP router, proxy, reverse proxy and to implement similar capabilities for other transports.","title":"Guide-level explanation"},{"location":"0003-linked-transports/#reference-level-explanation","text":"None.","title":"Reference-level explanation"},{"location":"0003-linked-transports/#drawbacks","text":"None.","title":"Drawbacks"},{"location":"0003-linked-transports/#rationale-and-alternatives","text":"The introduction of transports coupled with adding optional transport references to onramp and offramp specifications enables the tremor-runtime to act as a proxy or reverse proxy endpoint with logic implemented in the scripting or query language.","title":"Rationale and alternatives"},{"location":"0003-linked-transports/#prior-art","text":"None.","title":"Prior art"},{"location":"0003-linked-transports/#unresolved-questions","text":"This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation.","title":"Unresolved questions"},{"location":"0003-linked-transports/#future-possibilities","text":"None known at this time.","title":"Future possibilities"},{"location":"0004-sliding-window-mechanism/","text":"Feature Name: rfc_0004_sliding_window_mechanism Start Date: 2020-01-27 Issue: tremor-rs/tremor-rfcs#0007 RFC PR: tremor-rs/tremor-rfcs#0008 Summary \u00b6 The tremor-query language currently supports temporal window processing based on a provided system ( wall ) clock or data-driven intervals. Currently, however, the only supported windowing style in select statements are tumbling. There is no mechanism for tunbling windows where there may be multiple simultaneous (overlapping) windows. The sliding window mechanism corrects this. This RFC addresses these limitations by introducing a sliding window mechamism that can be configured with a number of steps. Motivation \u00b6 The tremor-query language cannot currently easily define windowing mechanics such as pair-wise comparisons or sliding data-driven windows that captures 'the last seconds worth' of data that update on an event by event basis. Sliding windows occur frequently in event processing algorithms and their addition to tremor-query is a natural extension to the language. Guide-level explanation \u00b6 Definition of a simple sliding window of step size 2 define sliding window pairs with size = 2 , end ; Application of the pairs window in a select statement select { \"window\" : window , \"count\" : stats :: count (), \"of\" : win :: collect ( event ), } from in [ pairs ] into out ; Sliding windows should work with tilt frames: select { \"window\" : window , \"count\" : stats :: count (), \"of\" : win :: collect ( event ), } from in [ sliding_pairs , tumbling_pairs , tumbling_triples , sliding_quads ] into out ; An interval based sliding window of duration 1 second: define sliding window last_second with interval = datetime :: with_seconds ( 1 ), end ; The addition of a sliding window mechanism is a fairly cosmetic language change but a fairly significant change to runtime facililities in window processing semantics and mechanics, grouping mechanism, tilt framing and may involve other enhancements or changes in order to manage memory pressure optimally. This RFC does not concern itself with implementation specifics. Reference-level explanation \u00b6 None. Drawbacks \u00b6 Sliding window mechanism uses relatively more memory when compared with tumbling windows and this should feature clearly in documentation and examples. Rationale and alternatives \u00b6 As described in the summary . Prior art \u00b6 Sliding windows are a common feature in CEP/ESP and aggregation systems. Unresolved questions \u00b6 This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation. Future possibilities \u00b6 None known at this time.","title":"0004-sliding-window-mechanism"},{"location":"0004-sliding-window-mechanism/#summary","text":"The tremor-query language currently supports temporal window processing based on a provided system ( wall ) clock or data-driven intervals. Currently, however, the only supported windowing style in select statements are tumbling. There is no mechanism for tunbling windows where there may be multiple simultaneous (overlapping) windows. The sliding window mechanism corrects this. This RFC addresses these limitations by introducing a sliding window mechamism that can be configured with a number of steps.","title":"Summary"},{"location":"0004-sliding-window-mechanism/#motivation","text":"The tremor-query language cannot currently easily define windowing mechanics such as pair-wise comparisons or sliding data-driven windows that captures 'the last seconds worth' of data that update on an event by event basis. Sliding windows occur frequently in event processing algorithms and their addition to tremor-query is a natural extension to the language.","title":"Motivation"},{"location":"0004-sliding-window-mechanism/#guide-level-explanation","text":"Definition of a simple sliding window of step size 2 define sliding window pairs with size = 2 , end ; Application of the pairs window in a select statement select { \"window\" : window , \"count\" : stats :: count (), \"of\" : win :: collect ( event ), } from in [ pairs ] into out ; Sliding windows should work with tilt frames: select { \"window\" : window , \"count\" : stats :: count (), \"of\" : win :: collect ( event ), } from in [ sliding_pairs , tumbling_pairs , tumbling_triples , sliding_quads ] into out ; An interval based sliding window of duration 1 second: define sliding window last_second with interval = datetime :: with_seconds ( 1 ), end ; The addition of a sliding window mechanism is a fairly cosmetic language change but a fairly significant change to runtime facililities in window processing semantics and mechanics, grouping mechanism, tilt framing and may involve other enhancements or changes in order to manage memory pressure optimally. This RFC does not concern itself with implementation specifics.","title":"Guide-level explanation"},{"location":"0004-sliding-window-mechanism/#reference-level-explanation","text":"None.","title":"Reference-level explanation"},{"location":"0004-sliding-window-mechanism/#drawbacks","text":"Sliding window mechanism uses relatively more memory when compared with tumbling windows and this should feature clearly in documentation and examples.","title":"Drawbacks"},{"location":"0004-sliding-window-mechanism/#rationale-and-alternatives","text":"As described in the summary .","title":"Rationale and alternatives"},{"location":"0004-sliding-window-mechanism/#prior-art","text":"Sliding windows are a common feature in CEP/ESP and aggregation systems.","title":"Prior art"},{"location":"0004-sliding-window-mechanism/#unresolved-questions","text":"This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation.","title":"Unresolved questions"},{"location":"0004-sliding-window-mechanism/#future-possibilities","text":"None known at this time.","title":"Future possibilities"},{"location":"0005-circuit-breaker-mechanism/","text":"Feature Name: rfc_0005_circuit_breaker_mechanism Start Date: 2020-01-27 Issue: tremor-rs/tremor-rfcs#0008 RFC PR: tremor-rs/tremor-rfcs#0009 Summary \u00b6 The tremor-runtime supports events from three different basic origins: * Events are user-defined business data events that arrive ultimately via Onramps and depart ultimately via Oframps * Signals are injected control events that are visible to operators and ramps. * Contraflow are injected control events that are visible to operators and ramps and travel in the contra-sense of primary flow. There is no mechanism to compensate for failures detected in externalising upstream or downstream components within the deployed graph of tremor artefacts. The circuit-breaker operator defines an event protocol that standardises how failure detection signals, events and actions are communicated across the tremor runtime. Motivation \u00b6 The absence of a standard and uniform circuit breaker interface prohibits authors of tremor-script / tremor-query from writing compensating logic and behaviours that are adaptive to failures in the runtime environment. The circuit breaker operator separates the signals, events and actions that are implied by circuit breakers with an operator that allows circuit breaker events to be leveraged by user-defined logic in the tremor-runtime regardless of the point of origin of those signals, events and actions. Guide-level explanation \u00b6 Definition of a circuit breaker with exponential backoff strategy and a rate based failure detector. define qos :: circuit_breaker operator cb with backoff = \"exponential\" , detector = \"success_rate_over_time_window\" , end Application of a circuit breaker to external inbound events from an external non-reliable data source create operator blue_smoke from cb ; select event from in into cb ; select event from cb / cb into out / cb ; Circuit Breaker signals: * Circuit breaker enabled * Circuit breaker disabled Circuit breakers events: * Opened - The circuit breaker has transitioned from 'closed' to 'opened' for a named external endpoint. * Closed - The circuit braeker has transitioned from 'opened' to 'closed' for a named external endpoint. Reference-level explanation \u00b6 None Drawbacks \u00b6 None Rationale and alternatives \u00b6 The introduction of circuit breakers enables finer grained control of compensating logic when external sources or sinks are detected as failed. The circuit breaker operator encapsulates runtime signals and contraflow so that circuit breaker's can be used simply in user defined logic. Prior art \u00b6 Circuit Breaker Design Pattern Crius Failsafe Unresolved questions \u00b6 A related concern in the tremor-runtime is backoff handling for back-pressure in downstream systems ( eg: Influx, ElasticSearch ). These use contraflow to propagate context to a back-pressure operator. There may be an opportunity to refactor the backpressure operator, separating out backoff handling logic. This should be considered by the implementor and this RFC updated accordingly. This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation. Future possibilities \u00b6 None known at this time.","title":"0005-circuit-breaker-mechanism"},{"location":"0005-circuit-breaker-mechanism/#summary","text":"The tremor-runtime supports events from three different basic origins: * Events are user-defined business data events that arrive ultimately via Onramps and depart ultimately via Oframps * Signals are injected control events that are visible to operators and ramps. * Contraflow are injected control events that are visible to operators and ramps and travel in the contra-sense of primary flow. There is no mechanism to compensate for failures detected in externalising upstream or downstream components within the deployed graph of tremor artefacts. The circuit-breaker operator defines an event protocol that standardises how failure detection signals, events and actions are communicated across the tremor runtime.","title":"Summary"},{"location":"0005-circuit-breaker-mechanism/#motivation","text":"The absence of a standard and uniform circuit breaker interface prohibits authors of tremor-script / tremor-query from writing compensating logic and behaviours that are adaptive to failures in the runtime environment. The circuit breaker operator separates the signals, events and actions that are implied by circuit breakers with an operator that allows circuit breaker events to be leveraged by user-defined logic in the tremor-runtime regardless of the point of origin of those signals, events and actions.","title":"Motivation"},{"location":"0005-circuit-breaker-mechanism/#guide-level-explanation","text":"Definition of a circuit breaker with exponential backoff strategy and a rate based failure detector. define qos :: circuit_breaker operator cb with backoff = \"exponential\" , detector = \"success_rate_over_time_window\" , end Application of a circuit breaker to external inbound events from an external non-reliable data source create operator blue_smoke from cb ; select event from in into cb ; select event from cb / cb into out / cb ; Circuit Breaker signals: * Circuit breaker enabled * Circuit breaker disabled Circuit breakers events: * Opened - The circuit breaker has transitioned from 'closed' to 'opened' for a named external endpoint. * Closed - The circuit braeker has transitioned from 'opened' to 'closed' for a named external endpoint.","title":"Guide-level explanation"},{"location":"0005-circuit-breaker-mechanism/#reference-level-explanation","text":"None","title":"Reference-level explanation"},{"location":"0005-circuit-breaker-mechanism/#drawbacks","text":"None","title":"Drawbacks"},{"location":"0005-circuit-breaker-mechanism/#rationale-and-alternatives","text":"The introduction of circuit breakers enables finer grained control of compensating logic when external sources or sinks are detected as failed. The circuit breaker operator encapsulates runtime signals and contraflow so that circuit breaker's can be used simply in user defined logic.","title":"Rationale and alternatives"},{"location":"0005-circuit-breaker-mechanism/#prior-art","text":"Circuit Breaker Design Pattern Crius Failsafe","title":"Prior art"},{"location":"0005-circuit-breaker-mechanism/#unresolved-questions","text":"A related concern in the tremor-runtime is backoff handling for back-pressure in downstream systems ( eg: Influx, ElasticSearch ). These use contraflow to propagate context to a back-pressure operator. There may be an opportunity to refactor the backpressure operator, separating out backoff handling logic. This should be considered by the implementor and this RFC updated accordingly. This RFC does not specify internals or implementation which is left to the implementor. The motiviating example should be sufficient to drive a suitable implementation.","title":"Unresolved questions"},{"location":"0005-circuit-breaker-mechanism/#future-possibilities","text":"None known at this time.","title":"Future possibilities"},{"location":"0006-plugin-development-kit/","text":"Feature Name: plugin_development_kit Start Date: (fill me in with today's date, YYYY-MM-DD) Tremor Issue: tremor-rs/tremor-runtime#0037 RFC PR: tremor-rs/tremor-rfcs#0010 Summary \u00b6 A plugin development kit ( PDK ) enables modularization of tremor components decoupling their software development lifecycle. The two main requirements for the PDK are loading shared linked libraries via a standard plugin mechanism that expose the plugin artifacts and refactoring internal component registries to allow referencing plugins. Motivation \u00b6 The first benefit of a PDK is to decouple the deployment of the tremor executable and components. This enables shipping, deploying, or updating artifacts dynamically after initial deployment. The second benefit is separating core and non-core or extended features development lifecycles. It stabilizes and standardizes how new artifacts are developed, shipped, tested, and deployed whilst normalizing packaging, operations and management. Lastly, rust compile times are high. Partitioning out components as plugins from the core runtime allows them to be compiled separately, and only when there are significant changes, reducing build times, and offering faster development cycles whilst simultaneously reducing overall compile times. Guide-level explanation \u00b6 The PDK supports plugins of the following kind: Onramps Offramps Codecs Preprocessors Postprocessors Operators Functions Extractors The resulting plugins can be loaded into a tremor instance either at start-time or dynamically and then used in deployments. Reference-level explanation \u00b6 The PDK requires extending registries for various artifacts; we need to enable registering additional artefacts in addition to the supporting builtin ones. Nested namespaces may be of benefit to prevent collisions. Plugin unloading needs careful consideration. In this revision, we are making unloading illegal to eliminate the complexity of dependency tracking and live usage tracking. Unloading a plugin in the initial implementation will require a restart of the runtime. Plugin lifecycle with support for etherealization, destruction and unloading is envisaged. A future RFC revision may replace this one for this purpose. Developer tooling such as template projects, traits, examples, and eventually, testing frameworks to facilitate higher developer experience for plugin developers is out of scope in this revision. A future RFC should cover off plugin developer experience once the PDK and a set of plugins have been implemented as concrete needs will become clearer over time. Drawbacks \u00b6 Plugins add deployment complexity. Currently tremor is a single binary. A binary with plugins introduces versioning and dependency management complexities. For example, when to allow/disallow multiple versions in the same process. Once the PDK is published, internal interfaces become public API surfaces. Binary compatibility, forward compatibility and separation of public from private or internal structures, types, behaviours and interfaces will be new concerns and constraints on the project. Plugin ownership and maintenance. Aside from code-related issues, we need a process for managing officially maintained plugins and for managing the promotion, demotion/deprecation and changes to maintership or ownership. The governance of plugins will require explicit consideration with respect to standards, processes and community governance. Rationale and alternatives \u00b6 The simple alternative to a PDK, is to internalize every artifact explicitly. This limits scalability and effectivness and is not tenable in the medium to long term. Another alternative is enhance tremor so that plugins can be 'soft coded' through a DSL. This may require extending existing languages, adding new DSLs and other changes to the tremor runtime. For some artifacts, such as codecs, or pre and post processors, this may be worth investigating. However, performance critical regions of the tremor runtime may limit the applicability of 'soft coded' plugins until the runtime evolves suitable APIs, facilities and development tooling. Webassembly might be another way to get and deploy additional code to tremor without the need of linked libraries, however interaction with existing third party libraries is unresolved. Prior art \u00b6 Java package name conventions libloading rust crate for dynamic library loading Java WebStart Mac OS X Universal Binaries WebAssembly wasmer WebAssembly runtime , wasmtime WebAssembly runtime Unresolved questions \u00b6 The impact of clustering on the PDK and plugin development and runtime lifecycle is unknown. As clustering support in tremor is in progress but not delivered at the time of writing, these questions are unexplored. Future possibilities \u00b6 Central plugin registry (eg: maven repository, cargo for crates, CPAN), cluster-aware PDK, bundles, and dependency/usage tracking, version management.","title":"0006-plugin-development-kit"},{"location":"0006-plugin-development-kit/#summary","text":"A plugin development kit ( PDK ) enables modularization of tremor components decoupling their software development lifecycle. The two main requirements for the PDK are loading shared linked libraries via a standard plugin mechanism that expose the plugin artifacts and refactoring internal component registries to allow referencing plugins.","title":"Summary"},{"location":"0006-plugin-development-kit/#motivation","text":"The first benefit of a PDK is to decouple the deployment of the tremor executable and components. This enables shipping, deploying, or updating artifacts dynamically after initial deployment. The second benefit is separating core and non-core or extended features development lifecycles. It stabilizes and standardizes how new artifacts are developed, shipped, tested, and deployed whilst normalizing packaging, operations and management. Lastly, rust compile times are high. Partitioning out components as plugins from the core runtime allows them to be compiled separately, and only when there are significant changes, reducing build times, and offering faster development cycles whilst simultaneously reducing overall compile times.","title":"Motivation"},{"location":"0006-plugin-development-kit/#guide-level-explanation","text":"The PDK supports plugins of the following kind: Onramps Offramps Codecs Preprocessors Postprocessors Operators Functions Extractors The resulting plugins can be loaded into a tremor instance either at start-time or dynamically and then used in deployments.","title":"Guide-level explanation"},{"location":"0006-plugin-development-kit/#reference-level-explanation","text":"The PDK requires extending registries for various artifacts; we need to enable registering additional artefacts in addition to the supporting builtin ones. Nested namespaces may be of benefit to prevent collisions. Plugin unloading needs careful consideration. In this revision, we are making unloading illegal to eliminate the complexity of dependency tracking and live usage tracking. Unloading a plugin in the initial implementation will require a restart of the runtime. Plugin lifecycle with support for etherealization, destruction and unloading is envisaged. A future RFC revision may replace this one for this purpose. Developer tooling such as template projects, traits, examples, and eventually, testing frameworks to facilitate higher developer experience for plugin developers is out of scope in this revision. A future RFC should cover off plugin developer experience once the PDK and a set of plugins have been implemented as concrete needs will become clearer over time.","title":"Reference-level explanation"},{"location":"0006-plugin-development-kit/#drawbacks","text":"Plugins add deployment complexity. Currently tremor is a single binary. A binary with plugins introduces versioning and dependency management complexities. For example, when to allow/disallow multiple versions in the same process. Once the PDK is published, internal interfaces become public API surfaces. Binary compatibility, forward compatibility and separation of public from private or internal structures, types, behaviours and interfaces will be new concerns and constraints on the project. Plugin ownership and maintenance. Aside from code-related issues, we need a process for managing officially maintained plugins and for managing the promotion, demotion/deprecation and changes to maintership or ownership. The governance of plugins will require explicit consideration with respect to standards, processes and community governance.","title":"Drawbacks"},{"location":"0006-plugin-development-kit/#rationale-and-alternatives","text":"The simple alternative to a PDK, is to internalize every artifact explicitly. This limits scalability and effectivness and is not tenable in the medium to long term. Another alternative is enhance tremor so that plugins can be 'soft coded' through a DSL. This may require extending existing languages, adding new DSLs and other changes to the tremor runtime. For some artifacts, such as codecs, or pre and post processors, this may be worth investigating. However, performance critical regions of the tremor runtime may limit the applicability of 'soft coded' plugins until the runtime evolves suitable APIs, facilities and development tooling. Webassembly might be another way to get and deploy additional code to tremor without the need of linked libraries, however interaction with existing third party libraries is unresolved.","title":"Rationale and alternatives"},{"location":"0006-plugin-development-kit/#prior-art","text":"Java package name conventions libloading rust crate for dynamic library loading Java WebStart Mac OS X Universal Binaries WebAssembly wasmer WebAssembly runtime , wasmtime WebAssembly runtime","title":"Prior art"},{"location":"0006-plugin-development-kit/#unresolved-questions","text":"The impact of clustering on the PDK and plugin development and runtime lifecycle is unknown. As clustering support in tremor is in progress but not delivered at the time of writing, these questions are unexplored.","title":"Unresolved questions"},{"location":"0006-plugin-development-kit/#future-possibilities","text":"Central plugin registry (eg: maven repository, cargo for crates, CPAN), cluster-aware PDK, bundles, and dependency/usage tracking, version management.","title":"Future possibilities"},{"location":"0007-pipeline-optimizations/","text":"Feature Name: pipeline-optimizations Start Date: 2020-01-29 Tremor Issue: tremor-rs/tremor-runtime#0033 RFC PR: tremor-rs/tremor-rfcs#0011 Summary \u00b6 As part of tremors execution engine, we transform the logic described in trickle query scripts into Directed Acyclic Graph (DAG) based pipelines. Each operation, operator, or action inside the query gets represented as a node in this graph. Every event passed through tremor traverses this graph of operators depth first. When an event arrives at an operator, this operator can alter, discard, or route the event to influence which following subgraph ( or subgraphs ) the event traverses afterward. The initial construction of the pipeline DAGs is naive and done in the most simplistic way possible to make extending/evolving it relatively painless during development. After the construction of the initial graph, it may undergo one or more transformations to optimize execution; for example, it may apply constant folding to migrate some runtime calculations to compile time where possible. This RFC aims to discuss these transformations and more complex transformations. As transformations may involve more than a single pass, and as tremors evolution may open new avenues for optimization, may introduce new domain languages, this RFC is not meant to be exhaustive but to refect the current and near-future state of optimizations. Motivation \u00b6 The executable pipeline is an integral part of tremor, quite literally every event that passes through it. This makes it a prime target for optimization as even small improvements, when gained on every event, sum up to significant gains. It is worth revisiting this topic regularly to see if additional cases present themselves. Problem case 1 \u00b6 As part of constructing the initial DAG we insert what we call pass through operators. They allow us to simplify the trickle language by not requiring all operators to have a connection logic or addressable name as well as form the edges of our DAG. Let us look at the following trickle script taken from the influx example in the docs and annotate it with passthrough operators and graph connections. # a passthrough[1] `in` is created # a passthrough[2] `out` is created # a passthrough[3] `err` is created define tumbling window ` 10 secs ` with interval = datetime :: with_seconds ( 10 ), end ; define tumbling window ` 1 min ` with interval = datetime :: with_minutes ( 1 ), end ; # a passthrough[4] `normalize` is created create stream normalize ; # a passthrough[5] `aggregate` is created create stream aggregate ; define generic :: batch operator batch with count = 3000 , timeout = 5 end ; create operator batch ; # the select operator is connected to the passthrough[1] `in` # and connects to the passthrough[5] `aggregate` select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); # the select operator is connected to the passthrough[5] `aggregate` # and connects to the passthrough[4] `normalize` select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; # the select operator is connected to the passthrough[4] `normalize` # and connects to the operator `batch` select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_{event.field}\" : event . stats . count , \"min_{event.field}\" : event . stats . min , \"max_{event.field}\" : event . stats . max , \"mean_{event.field}\" : event . stats . mean , \"stdev_{event.field}\" : event . stats . stdev , \"var_{event.field}\" : event . stats . var , \"p50_{event.field}\" : event . stats . percentiles [ \"0.5\" ], \"p90_{event.field}\" : event . stats . percentiles [ \"0.9\" ], \"p99_{event.field}\" : event . stats . percentiles [ \"0.99\" ], \"p99.9_{event.field}\" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; # This select statement itself is optimised to a passthrough[6] that # connects the operator `batch` with the passthrough[2] `out` select event from batch into out ; # This select statement itself is optimised to a passthrough[7] that # connects the passthrough[1] `in` with the passthrough[2] `out` select event from in into out ; To visualize the above, we can draw the graph as following where items in square brackets are passthrough operators, items in round brackets are 'active' operators and arrows are connections between them, double arrows represent the edges. => [in] -> (select 1) -> [aggregate] -> (select 2) -> [normalize] -> (select 3) -> (batch) -> [select 4] -> [out] => `-------------------------------------------------[select 5]----------------------------------------------' Other then connectivity, passthrough operators serve no direct value other than serving as a connection point. Looking at the example above, we can see that even a simple script like that can mean that an event traverses 6 passthrough operators and only 4 operators that affect the graph. As Passthrough operators do not modify the event, nor do they affect how the event traverses the graph, it is possible to remove them from the graph without any impact on the function of the graph itself. In result, the above graph could be rewritten as: => (select 1) -> (select 2) -> (select 3) -> (batch) => `---------------------------------------------------' Drawbacks \u00b6 Concerning Problem case 1 , we lose a one to one mapping between the script and the executable graph. It presents no further drawbacks. Rationale and alternatives \u00b6 Concerning Problem case 1 , an alternative approach for this would be not to introduce some of the pass-throughs in the first place. While in the short term this would yield the same results, there is a benefit to create a first a more verbose and general form and then reduce it down. This additional step makes it easier to apply other optimizations in later iterations. Prior art \u00b6 Graph rewriting Compiler optimizations Internally we are using similar techniques of rewriting parts of the tremor-script AST as part of the optimization step. Future possibilities \u00b6 The topic of pipeline optimization is never-ending endeavour as there are always further optimizations to be done. In the future, this could take the form of integration and interaction between different operators, extending pipeline level optimizations or go all the way to introducing a compiler. While those future possibilities might not be of direct concern for any case, it is important to keep them in mind to ensure optimizations done today do not block off possibilities in the future.","title":"0007-pipeline-optimizations"},{"location":"0007-pipeline-optimizations/#summary","text":"As part of tremors execution engine, we transform the logic described in trickle query scripts into Directed Acyclic Graph (DAG) based pipelines. Each operation, operator, or action inside the query gets represented as a node in this graph. Every event passed through tremor traverses this graph of operators depth first. When an event arrives at an operator, this operator can alter, discard, or route the event to influence which following subgraph ( or subgraphs ) the event traverses afterward. The initial construction of the pipeline DAGs is naive and done in the most simplistic way possible to make extending/evolving it relatively painless during development. After the construction of the initial graph, it may undergo one or more transformations to optimize execution; for example, it may apply constant folding to migrate some runtime calculations to compile time where possible. This RFC aims to discuss these transformations and more complex transformations. As transformations may involve more than a single pass, and as tremors evolution may open new avenues for optimization, may introduce new domain languages, this RFC is not meant to be exhaustive but to refect the current and near-future state of optimizations.","title":"Summary"},{"location":"0007-pipeline-optimizations/#motivation","text":"The executable pipeline is an integral part of tremor, quite literally every event that passes through it. This makes it a prime target for optimization as even small improvements, when gained on every event, sum up to significant gains. It is worth revisiting this topic regularly to see if additional cases present themselves.","title":"Motivation"},{"location":"0007-pipeline-optimizations/#problem-case-1","text":"As part of constructing the initial DAG we insert what we call pass through operators. They allow us to simplify the trickle language by not requiring all operators to have a connection logic or addressable name as well as form the edges of our DAG. Let us look at the following trickle script taken from the influx example in the docs and annotate it with passthrough operators and graph connections. # a passthrough[1] `in` is created # a passthrough[2] `out` is created # a passthrough[3] `err` is created define tumbling window ` 10 secs ` with interval = datetime :: with_seconds ( 10 ), end ; define tumbling window ` 1 min ` with interval = datetime :: with_minutes ( 1 ), end ; # a passthrough[4] `normalize` is created create stream normalize ; # a passthrough[5] `aggregate` is created create stream aggregate ; define generic :: batch operator batch with count = 3000 , timeout = 5 end ; create operator batch ; # the select operator is connected to the passthrough[1] `in` # and connects to the passthrough[5] `aggregate` select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); # the select operator is connected to the passthrough[5] `aggregate` # and connects to the passthrough[4] `normalize` select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; # the select operator is connected to the passthrough[4] `normalize` # and connects to the operator `batch` select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_{event.field}\" : event . stats . count , \"min_{event.field}\" : event . stats . min , \"max_{event.field}\" : event . stats . max , \"mean_{event.field}\" : event . stats . mean , \"stdev_{event.field}\" : event . stats . stdev , \"var_{event.field}\" : event . stats . var , \"p50_{event.field}\" : event . stats . percentiles [ \"0.5\" ], \"p90_{event.field}\" : event . stats . percentiles [ \"0.9\" ], \"p99_{event.field}\" : event . stats . percentiles [ \"0.99\" ], \"p99.9_{event.field}\" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; # This select statement itself is optimised to a passthrough[6] that # connects the operator `batch` with the passthrough[2] `out` select event from batch into out ; # This select statement itself is optimised to a passthrough[7] that # connects the passthrough[1] `in` with the passthrough[2] `out` select event from in into out ; To visualize the above, we can draw the graph as following where items in square brackets are passthrough operators, items in round brackets are 'active' operators and arrows are connections between them, double arrows represent the edges. => [in] -> (select 1) -> [aggregate] -> (select 2) -> [normalize] -> (select 3) -> (batch) -> [select 4] -> [out] => `-------------------------------------------------[select 5]----------------------------------------------' Other then connectivity, passthrough operators serve no direct value other than serving as a connection point. Looking at the example above, we can see that even a simple script like that can mean that an event traverses 6 passthrough operators and only 4 operators that affect the graph. As Passthrough operators do not modify the event, nor do they affect how the event traverses the graph, it is possible to remove them from the graph without any impact on the function of the graph itself. In result, the above graph could be rewritten as: => (select 1) -> (select 2) -> (select 3) -> (batch) => `---------------------------------------------------'","title":"Problem case 1"},{"location":"0007-pipeline-optimizations/#drawbacks","text":"Concerning Problem case 1 , we lose a one to one mapping between the script and the executable graph. It presents no further drawbacks.","title":"Drawbacks"},{"location":"0007-pipeline-optimizations/#rationale-and-alternatives","text":"Concerning Problem case 1 , an alternative approach for this would be not to introduce some of the pass-throughs in the first place. While in the short term this would yield the same results, there is a benefit to create a first a more verbose and general form and then reduce it down. This additional step makes it easier to apply other optimizations in later iterations.","title":"Rationale and alternatives"},{"location":"0007-pipeline-optimizations/#prior-art","text":"Graph rewriting Compiler optimizations Internally we are using similar techniques of rewriting parts of the tremor-script AST as part of the optimization step.","title":"Prior art"},{"location":"0007-pipeline-optimizations/#future-possibilities","text":"The topic of pipeline optimization is never-ending endeavour as there are always further optimizations to be done. In the future, this could take the form of integration and interaction between different operators, extending pipeline level optimizations or go all the way to introducing a compiler. While those future possibilities might not be of direct concern for any case, it is important to keep them in mind to ensure optimizations done today do not block off possibilities in the future.","title":"Future possibilities"},{"location":"0008-onramp-postgres/","text":"Feature Name: rfc_0008_onramp_postgres Start Date: 2020-01-21 Tremor Issue: tremor-rs/tremor-runtime#0015 RFC PR: tremor-rs/tremor-rfcs#0008 Summary \u00b6 Pull data from Postgres tables. Motivation \u00b6 We see use cases a function of features supported by PostgreSQL out of the box, as well as additional features introduced as extensions or otherwise packaged as completely separate products. Primary examples that we are looking at are TimescaleDB and PipelineDB . The ultimate goal is to be able to reliably pipe in data from PostgreSQL core to Tremor engine. Guide-level explanation \u00b6 PostgreSQL is a standard Tremor onramp. It is configured by passing in standard PostgreSQL connection string arguments, such as host , port , user , password and dbname . In addition to connection string arguments, we require: * interval in milliseconds that specifies the time we wait before executing the next query * consume_from specifies the timestamp from which to backfill data * query in form of standard SELECT query which retrieves rows from a table or view Unlike other onramps implemented so far, PostgreSQL onramp will persist consume_from and consume_until , allowing recovery from error conditions or other failures. Example onramp.yml : id: db type: postgres codec: json config: host: localhost port: 5432 user: postgres password: example query: \"SELECT id, name from events WHERE produced_at <= $1 AND produced_at > $2\" interval_ms: 1000 dbname: sales cache: path: \"/path/to/cache.json\" size: 4096 Cache is a memory mapped region, either file-based or anonymous. Reference-level explanation \u00b6 Drawbacks \u00b6 The onramp does not provide an implementation of all types Postgres supports and the entirety of query language (for example, LISTEN/NOTIFY semantics). Rationale and alternatives \u00b6 As a starting point, or first implementation, supporting basic SELECT statements with a time interval as additional WHERE clause supports the major use case: ingesting rows from a table or view. Alternative approach would utilize PostgreSQL single row mode suitable for ingestion of results returned by queries that span a large number of rows. Ensuring at-most-once ingestion of rows would be a potential problem and a time-consuming/API-breaking implementation. Other approaches, such as trigger-based watches or binary log readers are also a possibility. Prior art \u00b6 Data Integration Unresolved questions \u00b6 Future possibilities \u00b6 For users that utilize PostgreSQL as an event store, which seems to be a more common use case lately, support for LISTEN/NOTIFY would be essential to become one of modes of this onramp. Support multiple queries in the same onramp. More flexible means of specifying parameters.","title":"0008-onramp-postgres"},{"location":"0008-onramp-postgres/#summary","text":"Pull data from Postgres tables.","title":"Summary"},{"location":"0008-onramp-postgres/#motivation","text":"We see use cases a function of features supported by PostgreSQL out of the box, as well as additional features introduced as extensions or otherwise packaged as completely separate products. Primary examples that we are looking at are TimescaleDB and PipelineDB . The ultimate goal is to be able to reliably pipe in data from PostgreSQL core to Tremor engine.","title":"Motivation"},{"location":"0008-onramp-postgres/#guide-level-explanation","text":"PostgreSQL is a standard Tremor onramp. It is configured by passing in standard PostgreSQL connection string arguments, such as host , port , user , password and dbname . In addition to connection string arguments, we require: * interval in milliseconds that specifies the time we wait before executing the next query * consume_from specifies the timestamp from which to backfill data * query in form of standard SELECT query which retrieves rows from a table or view Unlike other onramps implemented so far, PostgreSQL onramp will persist consume_from and consume_until , allowing recovery from error conditions or other failures. Example onramp.yml : id: db type: postgres codec: json config: host: localhost port: 5432 user: postgres password: example query: \"SELECT id, name from events WHERE produced_at <= $1 AND produced_at > $2\" interval_ms: 1000 dbname: sales cache: path: \"/path/to/cache.json\" size: 4096 Cache is a memory mapped region, either file-based or anonymous.","title":"Guide-level explanation"},{"location":"0008-onramp-postgres/#reference-level-explanation","text":"","title":"Reference-level explanation"},{"location":"0008-onramp-postgres/#drawbacks","text":"The onramp does not provide an implementation of all types Postgres supports and the entirety of query language (for example, LISTEN/NOTIFY semantics).","title":"Drawbacks"},{"location":"0008-onramp-postgres/#rationale-and-alternatives","text":"As a starting point, or first implementation, supporting basic SELECT statements with a time interval as additional WHERE clause supports the major use case: ingesting rows from a table or view. Alternative approach would utilize PostgreSQL single row mode suitable for ingestion of results returned by queries that span a large number of rows. Ensuring at-most-once ingestion of rows would be a potential problem and a time-consuming/API-breaking implementation. Other approaches, such as trigger-based watches or binary log readers are also a possibility.","title":"Rationale and alternatives"},{"location":"0008-onramp-postgres/#prior-art","text":"Data Integration","title":"Prior art"},{"location":"0008-onramp-postgres/#unresolved-questions","text":"","title":"Unresolved questions"},{"location":"0008-onramp-postgres/#future-possibilities","text":"For users that utilize PostgreSQL as an event store, which seems to be a more common use case lately, support for LISTEN/NOTIFY would be essential to become one of modes of this onramp. Support multiple queries in the same onramp. More flexible means of specifying parameters.","title":"Future possibilities"},{"location":"0009-ramp-interface/","text":"Feature Name: ramp-interface Start Date: 2020-03-09 Tremor Issue: tremor-rs/tremor-runtime#0108 TBD RFC PR: tremor-rs/tremor-rfcs#0018 Summary \u00b6 This RFC proposes a generalized interface for onramps(sources) and offramps(sinks). This interface can serve as a basis for the PDK as it unifies how tremor core addresses ramps. As a second benefit, it could serve as a de facto standard for sources and sinks in the broader rust event processing ecosystem. Motivation \u00b6 The RFC 0006 outlines the need and plan to implement a plugin development kit allow decoupling parts of tremor to make development less centered around a single artifact. To enable plugins, plugins of the same type need to share a standard interface over which they communicate with the outside world. As of today, ramps are the least standardized component in tremor where each ramp, in no small degree, \"does its own thing.\" A standardized interface paves the way to implement ramps in the PDK. The secondary motivation is that there is an emerging event processing ecosystem in the rust world, providing a standardized interface that helps code reusability, quality, and sharing. For example, sharing code with timber.io's Vector project could become possible. Guide-level explanation \u00b6 We introduce a Sink and a Source trait. Those traits abstract over the following parts: configuration data handling status handling (errors, failures, backpressure for sinks) lifetime management (initialization and shutdown) event handoff (either to or from the onramp) circuit breaker control/signals/events support for guaranteed delivery These interface changes trigger a redesign of the current codecs and pre/post processors as they would likely be outside of the scope of a sink or source. Reference-level explanation \u00b6 We should discuss the communication design at this part to provide reasoning as to why it is sound. To enable circuit breakers and guaranteed delivery, we need to communicate from a source through a pipeline to an offramp. This model comes with a challenge of a possible live lock at the source and pipeline level. To elaborate, we look at the source as an example. The loop of communication creates the possibility that the source sends an event to the pipeline, blocking at a full queue. At the same time, the pipeline sends an insight to the source, also blocking at a full queue and thus locking forever. To resolve the lock, we use an unbounded queue for insights. Without additional guarantees, unbounded queues hold the risk of uncontrolled risk. In the implementation we guard against that in two ways so we can guarantee a sound implementation: In both pipelines and sources, we always process insights first, so forward flow only happens when there is no contraflow remaining to process. By that, we guarantee that before we send an event, we drain all pending insights (and their unbounded queues). Events create insights. Neither sinks nor pipelines can create an insight without a corresponding event. If no events flow forward, there is a finite number of insights to be processed, so while we use unbounded queues, we bound the number of insights that can exist between two events. Drawbacks \u00b6 Generalized sinks and sources are less specialized than custom-built ones. The chances are good that it complicates some of the implementations and can have a slight negative impact on performance. The implementation lends itself to adopt an async based implementation. A move from threads to async tasks puts a performance penalty on single pipeline deployments. The trade-off here is that an async implementation lends itself to higher concurrency situations and is less impacted when there are many more pipelines or ramps than available cores. Looking down the roadmap, this does provide the characteristics we need for clustering. To put this in numbers here, a comparison of 0.8.0 compared with the async implementation. To do this, we ran the test with three physical cores and 42 threads (24 cores + 24 threads). We then run both 0.8.0 and current with 1, 2, 4, 8, 16, 32, and 64 pipelines and onramps against a single blackhole offramp and the throughput recorded. MB/s 0.8 - 3 cur - 3 0.8 - 48 cur - 48 1 346.3 273.8 155.4 128.6 2 303.9 339.1 261.2 292.3 4 210.8 342.7 175.2 234.2 8 78.2 308.3 163.6 209.9 16 27.0 265.6 174.1 210.9 32 8.2 245.9 173.0 201.2 64 1.4 229.8 154.4 197.4 Rationale and alternatives \u00b6 First of all, not generalizing ramps excludes them from the PDK. The second rationale and alternative is the choice between a strictly threaded and an asynchronous model already described in the drawbacks section. It also details why the negative impact on single-threaded performance is a worthwhile trade-off. Prior art \u00b6 Generalized interfaces are a typical pattern. One example is the ring , a common abstraction over web applications that allows the reuse of shared parts and logic. Other applications and domains use the same principle to create a more extensive ecosystem around a concept. Unresolved questions \u00b6 This RFC does not address linked on/offramps since they are a particular case. Future possibilities \u00b6 A further opportunity is to extend the concept of generalized ramps to linked on and offramps. For the time being, Sinks and Sources exist as abstractions under the Onramp / Offramp model. We can remove this abstraction when we have ensured the model works with linked ramps.","title":"0009-ramp-interface"},{"location":"0009-ramp-interface/#summary","text":"This RFC proposes a generalized interface for onramps(sources) and offramps(sinks). This interface can serve as a basis for the PDK as it unifies how tremor core addresses ramps. As a second benefit, it could serve as a de facto standard for sources and sinks in the broader rust event processing ecosystem.","title":"Summary"},{"location":"0009-ramp-interface/#motivation","text":"The RFC 0006 outlines the need and plan to implement a plugin development kit allow decoupling parts of tremor to make development less centered around a single artifact. To enable plugins, plugins of the same type need to share a standard interface over which they communicate with the outside world. As of today, ramps are the least standardized component in tremor where each ramp, in no small degree, \"does its own thing.\" A standardized interface paves the way to implement ramps in the PDK. The secondary motivation is that there is an emerging event processing ecosystem in the rust world, providing a standardized interface that helps code reusability, quality, and sharing. For example, sharing code with timber.io's Vector project could become possible.","title":"Motivation"},{"location":"0009-ramp-interface/#guide-level-explanation","text":"We introduce a Sink and a Source trait. Those traits abstract over the following parts: configuration data handling status handling (errors, failures, backpressure for sinks) lifetime management (initialization and shutdown) event handoff (either to or from the onramp) circuit breaker control/signals/events support for guaranteed delivery These interface changes trigger a redesign of the current codecs and pre/post processors as they would likely be outside of the scope of a sink or source.","title":"Guide-level explanation"},{"location":"0009-ramp-interface/#reference-level-explanation","text":"We should discuss the communication design at this part to provide reasoning as to why it is sound. To enable circuit breakers and guaranteed delivery, we need to communicate from a source through a pipeline to an offramp. This model comes with a challenge of a possible live lock at the source and pipeline level. To elaborate, we look at the source as an example. The loop of communication creates the possibility that the source sends an event to the pipeline, blocking at a full queue. At the same time, the pipeline sends an insight to the source, also blocking at a full queue and thus locking forever. To resolve the lock, we use an unbounded queue for insights. Without additional guarantees, unbounded queues hold the risk of uncontrolled risk. In the implementation we guard against that in two ways so we can guarantee a sound implementation: In both pipelines and sources, we always process insights first, so forward flow only happens when there is no contraflow remaining to process. By that, we guarantee that before we send an event, we drain all pending insights (and their unbounded queues). Events create insights. Neither sinks nor pipelines can create an insight without a corresponding event. If no events flow forward, there is a finite number of insights to be processed, so while we use unbounded queues, we bound the number of insights that can exist between two events.","title":"Reference-level explanation"},{"location":"0009-ramp-interface/#drawbacks","text":"Generalized sinks and sources are less specialized than custom-built ones. The chances are good that it complicates some of the implementations and can have a slight negative impact on performance. The implementation lends itself to adopt an async based implementation. A move from threads to async tasks puts a performance penalty on single pipeline deployments. The trade-off here is that an async implementation lends itself to higher concurrency situations and is less impacted when there are many more pipelines or ramps than available cores. Looking down the roadmap, this does provide the characteristics we need for clustering. To put this in numbers here, a comparison of 0.8.0 compared with the async implementation. To do this, we ran the test with three physical cores and 42 threads (24 cores + 24 threads). We then run both 0.8.0 and current with 1, 2, 4, 8, 16, 32, and 64 pipelines and onramps against a single blackhole offramp and the throughput recorded. MB/s 0.8 - 3 cur - 3 0.8 - 48 cur - 48 1 346.3 273.8 155.4 128.6 2 303.9 339.1 261.2 292.3 4 210.8 342.7 175.2 234.2 8 78.2 308.3 163.6 209.9 16 27.0 265.6 174.1 210.9 32 8.2 245.9 173.0 201.2 64 1.4 229.8 154.4 197.4","title":"Drawbacks"},{"location":"0009-ramp-interface/#rationale-and-alternatives","text":"First of all, not generalizing ramps excludes them from the PDK. The second rationale and alternative is the choice between a strictly threaded and an asynchronous model already described in the drawbacks section. It also details why the negative impact on single-threaded performance is a worthwhile trade-off.","title":"Rationale and alternatives"},{"location":"0009-ramp-interface/#prior-art","text":"Generalized interfaces are a typical pattern. One example is the ring , a common abstraction over web applications that allows the reuse of shared parts and logic. Other applications and domains use the same principle to create a more extensive ecosystem around a concept.","title":"Prior art"},{"location":"0009-ramp-interface/#unresolved-questions","text":"This RFC does not address linked on/offramps since they are a particular case.","title":"Unresolved questions"},{"location":"0009-ramp-interface/#future-possibilities","text":"A further opportunity is to extend the concept of generalized ramps to linked on and offramps. For the time being, Sinks and Sources exist as abstractions under the Onramp / Offramp model. We can remove this abstraction when we have ensured the model works with linked ramps.","title":"Future possibilities"},{"location":"0010-modularity/","text":"Feature Name: rfc-0010-modularity Start Date: 2020-04-02 Tremor Issue: tremor-rs/tremor-runtime#0174 RFC PR: tremor-rs/tremor-rfcs#0021 Summary \u00b6 Provide mechanisms for sharing, reuse and composition of user-defined logic in tremor. Motivation \u00b6 As user-defined logic deployed with tremor script and query pipelines become more complex and larger ( by significant lines of code ) mechanisms that favour better composition, reuse and sharing of user defined logic and queries are needed. Currently the unit of modularity and unit of deployment in tremor are indivisible blobs of code - be they scripts or queries. This has worked very well to date but as the complexity and size of solutions built with tremor grows this is no longer tenable in the long term. Identified requirements As tremor has multiple DSLs, the building blocks for reusing units of code from the file system should be common across DSLs. For tremor-script, code should be modularisable through the introduction of functions. For tremor-query, code should be modularisable through modular definitions and/or reusable sub-queries Modules should be nestable on the file system, and within DSLs with a consistent means to reference units of code or values defined within nested modules regardless of their origin ( within the same unit of code, from some external module ). There should be a mechanism to load external libraries or packages of code from well-known locations. The module mechanism should be usable by multiple DSLs with minimal effort and with the same basic behaviour, structure. However, this RFC does not place constraints on any DSL specifics per se. Guide-level explanation \u00b6 Elements of modular user defined logic in the tremor project. Module Path \u00b6 A module path is a set of URLs ( normatively directories on a file system ) that form the root of a set of related modules. On Linux/Unix module paths are provided via the TREMOR_PATH environment variable and they are separated by ':' ( colon ). Paths that are not readable or that do not exist are ignored. Example path: TREMOR_PATH = \"/etc/tremor/lib:/opt/shared/framework/lib:/opt/myproject/mylib\" Modules \u00b6 Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file nesting is via the mod clause. In tremor-script, only the top-level module can capture events or or mutate state. Modules loaded via the module system are restricted to const, fn, and intrinsic expressions. By design constraint at this time, tremor-script is biased towards pure side-effect free functional programming. In tremor-query, only the top-level module can create nodes in the active query pipeline graph. A module logically encapsulates a reusable sub-graph in a query pipeline. The definitions of windows, operators or scripts can be reused. Within embedded scripts, modules used in scripts are constrained to the rules for modules for tremor-script. In addition tremor-script modules can be included in trickle files to expose their functions and constants for use in select , group by , having and where . In both the tremor-script and tremor-query DSLs, modules can be defined physically on the file system. For example given the following modular hierarchy configured on the module path: +-- foo +-- bar +-- snot.tremor +-- baz +-- badger.tremor A modular tremor-script can refer to the constant values as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.tremor' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.tremor' let c = \"{snot::snot}{badger::badger}\" ; # fully qualified references c The same module hierarchy can be created in a tremor file directly as follows: mod foo with mod bar with const snot = \"beep\" ; end ; mod baz with const badger = \"boop\" ; end ; end ; let snot = foo :: bar :: snot ; let badger = foo :: baz :: badger ; \"{snot}-{badger}\" ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; \"Hello {fleek::snot}\" Modules in tremor query follow the same semantics and behaviour with respect to physical versus inline definition, aliasing to avoid naming scope clashes. It is to be noted that inclusion via use will prevent circular inclusion as in file a.tremor can use b.tremor but at that point b.tremor can no longer use a.tremor as this would create a circle. This is a restriction of the current implementation and may or may not be relaxed in the future. Preprocessor \u00b6 In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users. This PR introduces two preprocessor directives: <byte offset> <line> <column> <compilation unit> <filename> This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line macro is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future. <key> = <const-expr> Pipeline level configuration in trickle, this allows setting compile time pipeline configuration such as metrics_interval_s . Preprocessing our script from the module section produces a single consolidated source file as follows: #!line 0 0 0 1 ./foo/bar/snot.tremor mod snot with #!line 0 0 0 1 ./foo/bar/snot.tremor const snot = \"beep\" ; end ; #!line 19 1 0 0 script.tremor #!line 0 0 0 2 ./foo/baz/badger.tremor mod badger with #!line 0 0 0 2 ./foo/baz/badger.tremor const badger = \"boop\" ; end ; #!line 41 1 0 0 script.tremor let c = \"{snot::snot}{badger::badger}\" ; emit c Functions \u00b6 While constants in modules offer the ability to have reusable data, functions allow for reusable logic. Functions are expression-based - so every function returns a data value. Functions cannot manipulate or mutate events, metadata or state. Side effecting operations to the data flow through a script such as the emit or drop keywords are also not allowed in functions. Recursion, specifically tail recursion, is supported in functions but a maximum recursion depth (of currently 1024 by default) is imposed. The limit can be changed in tremor-server using the --recursion-limit LIMIT argument. As tremor is primarily an event processing engine there are no facilities for user defined logic to loop or recurse infinitely. Recursion is indicated by the recur expression that gets passed data from the current iteration as arguments for the following invocation. Functions may access constants but cannot access external mutable state. Functions are limited to only call functions that were defined prior to themselves, this limits the risk of cyclic recursion between multiple functions and ensure that every call is guaranteed to terminate. Functions come in multiple forms: \u00b6 Intrinsic functions \u00b6 Intrinsic functions provide the signature of a builtin function. These are provided for documentation purposes and so that API documentation can be provided for builtin functions in the same way as user defined functions. ### The `test` module is used for writing tremor unit ### tests. ## Runs an assertion for a test, ensures that `expected` and ## `got` are the ## same. If not errors. ## ## **WARNING**: Do not run assertions in production code! ## ## Returns an `bool`. intrinsic fn assert ( name , expected , got ) as test :: assert ; Ordinary functions \u00b6 Of the form fn <name>([<args>][,...]) with provide named arguments with optional variable arguments through the ellipses ... or varargs operator. Varargs are stored in the args array. The ordinary form does not support partial functions. An ordinary function wrapping a call to a tail recursive fibonacci function: fn fib ( n ) with fib_ ( 0 , 1 , n ) end ; fib ( 7 ); # Call locally defined function fib Matching Functions \u00b6 Matching functions using fn <name>(<args>) of followed by case expressions and an optional default statement that match. The matching function form imposes a default case requirement so that unmatches cases have error handling defined. Unlike match expressions the default case in user defined functions must not ( and can not ) be omitted. A contrived example showing math functions with value matching, extractor matching and function case guards. use std :: type ; fn snottify ( s ) of case ( \"badger\" ) => \"snot badger, hell yea!\" case (\\~ json ||) => let s . snot = true , s case ( s ) when type :: is_string ( s ) => \"snot {s}\" default => \"snot caller, you can't snottify that!\" end ; Recursive Functions \u00b6 Tail recursive functions follow the signature of the function over which recursion is being invoked and use a recur(<args>) call expression. If the signature of a recursive call supports partial function semantics then this is respected under tail recursion. If the signature of a recursive call supports varargs semantics then this is respected under tail recursion. A tail-recursive implementation of fibonacci called by fib(n) above: fn fib_ ( a , b , n ) of case ( a , b , n ) when n > 0 => recur ( b , a + b , n - 1 ) default => a end ; Limitations and constraints: \u00b6 Functions can only be defined with a singular arity \u00b6 Functions currently can not be redefined with multiple arities. So a function foo(n) precludes a second definition of a function called foo with two or more arguments. However the function foo(n,...) defines a function that can take one or more arguments. This constraint may be lifted in the future once usage and adoption favor enhancing functionality. Higher Order functions are not supported \u00b6 As the type system underpinning tremor-script and tremor-query does not support expression or function references, higher order functions are thus not supported at this time. Hardcoded Recursion Depth \u00b6 Although functions are tail-recursive and stack limits are not a functional concern, the tremor event processing system is primarily designed for event streaming applications. A recursion depth is imposed to prevent functions from recursing indefinitely and blocking event streams from progressing. This is a feature, not a constraint. But it is important to be aware of when developing will behaved functions. At this point in time the maximum depth is 1024 and can not be changed without recompiling tremor. Tuple patterns \u00b6 As a side effect of adding functions this RFC introduces tuple patterns. Internally they are used to implement Matching functions but are available for use in match statements as well. Tuple patterns are written in the form of %(<pattern 1>, <pattern 2>) for patterns with a fixed number of elements or %(<pattern 1>, <pattern 2>, ...) with literal ... for open patterns. Tuple patterns with a fixed number of elements match arrays, with the same number of elements where each element matches the pattern in the same place. So the first pattern must match the first element of the array, the second pattern the second element and so on. An open pattern matches an array that is at lest the same number of elements as the pattern but can have more otherwise the rules are the same. There is also a new pattern introduced to predicate patterns which is the \"I don't care\" pattern _ which will match every element on an array. So: let o = origin :: as_uri_record (); match o of # matches for the path: # \"/api/v1/get/<something>\" # \"/api/v1/get/<something>/snot\" # \"/api/v1/get/<something>/snot/badger\" # and so one case %( \"api\" , \"v1\" , \"get\" , _ , ...) => \"get request\" end ; Reference-level explanation \u00b6 The module path, modules and use rule provide the language and runtime agnostic core facilities that allow queries in tremor-query and code in tremor-script to be namespaced logically via the mod syntax and physically on file systems. A new lexical preprocessing phase parses out occurrences of use rules in scripts, queries and embedded scripts replacing them with preprocessing directives and the contents of referenced modules. The parser in modular scripts and queries must now keep track of relative and absolute module scope. As support for both logical namespaces via the mod syntax and physical isolation through the file system and module path is supported, an external module effectively defines a module namespace. So a source file foo.trickle at the root of the module path would ordinarily be used to include definitions into another query with a use foo; declaration at the head of the file. Nested directories also form namespaces so bar/baz/snot.trickle would be declared for use as use bar::baz::snot . The same layout and rules apply for scripts so bar/baz/snot.trickle would be declared for use as use bar::baz::snot . Where modules of the same base name in the physical file system such as 'badger.tremor' and 'badger.trickle' are present in the same module path behaviour of the runtime is currently undefined. The prototype implementation of modules in tremor that accompanies this RFC gracefully handle the situation for embedded tremor scripts within trickle query files. As a convenience to developers, developer tools such as tremor-tool , tremor-script and tremor-query automatically included the current working directory as a mount point in the module path. If a TREMOR_PATH environment variable is set then it overrides any default behaviour. In tremor-server TREMOR_PATH is required to be set or no include path will be available. This is reflected in the docker image. Drawbacks \u00b6 Modularising logic in tremor increases complexity of the engine and runtime, however the relative increase in complexity is perceived as negligible given the value gained by developers by introducing the facility. At this time higher order functions are not supported as the tremor type system is constrained to JSON compatible value types and introducing module or function references would make the type system asymmetric with JSON. This is left for a future RFC. At this time the query language supports only modularising definitions of windows, operators or scripts. The creation of query language pipeline graph nodes or link of nodes in a graph is not supported in external modules. Rationale and alternatives \u00b6 The design of the module mechanism and its application to tremor-script and tremor-query provide the highest degree of reuse whilst imposing the lowest runtime impact today and without closing off opportunities for evolving and improving the mechanisms in future. Prior art \u00b6 This RFC and it's implementation draws inspiration from the C preprocessor as well as the application of use in Rust, and the functional pattern matching style from the Erlang programming language. Unresolved questions \u00b6 None. Future possibilities \u00b6 Currently var arg functions do not combine with either match or recursion or matching functions. This presents a good future opportunity for extending functions. Another future possibility is to expand the capabilities of use in trickle to include full sub queries.","title":"0010-modularity"},{"location":"0010-modularity/#summary","text":"Provide mechanisms for sharing, reuse and composition of user-defined logic in tremor.","title":"Summary"},{"location":"0010-modularity/#motivation","text":"As user-defined logic deployed with tremor script and query pipelines become more complex and larger ( by significant lines of code ) mechanisms that favour better composition, reuse and sharing of user defined logic and queries are needed. Currently the unit of modularity and unit of deployment in tremor are indivisible blobs of code - be they scripts or queries. This has worked very well to date but as the complexity and size of solutions built with tremor grows this is no longer tenable in the long term. Identified requirements As tremor has multiple DSLs, the building blocks for reusing units of code from the file system should be common across DSLs. For tremor-script, code should be modularisable through the introduction of functions. For tremor-query, code should be modularisable through modular definitions and/or reusable sub-queries Modules should be nestable on the file system, and within DSLs with a consistent means to reference units of code or values defined within nested modules regardless of their origin ( within the same unit of code, from some external module ). There should be a mechanism to load external libraries or packages of code from well-known locations. The module mechanism should be usable by multiple DSLs with minimal effort and with the same basic behaviour, structure. However, this RFC does not place constraints on any DSL specifics per se.","title":"Motivation"},{"location":"0010-modularity/#guide-level-explanation","text":"Elements of modular user defined logic in the tremor project.","title":"Guide-level explanation"},{"location":"0010-modularity/#module-path","text":"A module path is a set of URLs ( normatively directories on a file system ) that form the root of a set of related modules. On Linux/Unix module paths are provided via the TREMOR_PATH environment variable and they are separated by ':' ( colon ). Paths that are not readable or that do not exist are ignored. Example path: TREMOR_PATH = \"/etc/tremor/lib:/opt/shared/framework/lib:/opt/myproject/mylib\"","title":"Module Path"},{"location":"0010-modularity/#modules","text":"Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file nesting is via the mod clause. In tremor-script, only the top-level module can capture events or or mutate state. Modules loaded via the module system are restricted to const, fn, and intrinsic expressions. By design constraint at this time, tremor-script is biased towards pure side-effect free functional programming. In tremor-query, only the top-level module can create nodes in the active query pipeline graph. A module logically encapsulates a reusable sub-graph in a query pipeline. The definitions of windows, operators or scripts can be reused. Within embedded scripts, modules used in scripts are constrained to the rules for modules for tremor-script. In addition tremor-script modules can be included in trickle files to expose their functions and constants for use in select , group by , having and where . In both the tremor-script and tremor-query DSLs, modules can be defined physically on the file system. For example given the following modular hierarchy configured on the module path: +-- foo +-- bar +-- snot.tremor +-- baz +-- badger.tremor A modular tremor-script can refer to the constant values as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.tremor' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.tremor' let c = \"{snot::snot}{badger::badger}\" ; # fully qualified references c The same module hierarchy can be created in a tremor file directly as follows: mod foo with mod bar with const snot = \"beep\" ; end ; mod baz with const badger = \"boop\" ; end ; end ; let snot = foo :: bar :: snot ; let badger = foo :: baz :: badger ; \"{snot}-{badger}\" ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; \"Hello {fleek::snot}\" Modules in tremor query follow the same semantics and behaviour with respect to physical versus inline definition, aliasing to avoid naming scope clashes. It is to be noted that inclusion via use will prevent circular inclusion as in file a.tremor can use b.tremor but at that point b.tremor can no longer use a.tremor as this would create a circle. This is a restriction of the current implementation and may or may not be relaxed in the future.","title":"Modules"},{"location":"0010-modularity/#preprocessor","text":"In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users. This PR introduces two preprocessor directives: <byte offset> <line> <column> <compilation unit> <filename> This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line macro is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future. <key> = <const-expr> Pipeline level configuration in trickle, this allows setting compile time pipeline configuration such as metrics_interval_s . Preprocessing our script from the module section produces a single consolidated source file as follows: #!line 0 0 0 1 ./foo/bar/snot.tremor mod snot with #!line 0 0 0 1 ./foo/bar/snot.tremor const snot = \"beep\" ; end ; #!line 19 1 0 0 script.tremor #!line 0 0 0 2 ./foo/baz/badger.tremor mod badger with #!line 0 0 0 2 ./foo/baz/badger.tremor const badger = \"boop\" ; end ; #!line 41 1 0 0 script.tremor let c = \"{snot::snot}{badger::badger}\" ; emit c","title":"Preprocessor"},{"location":"0010-modularity/#functions","text":"While constants in modules offer the ability to have reusable data, functions allow for reusable logic. Functions are expression-based - so every function returns a data value. Functions cannot manipulate or mutate events, metadata or state. Side effecting operations to the data flow through a script such as the emit or drop keywords are also not allowed in functions. Recursion, specifically tail recursion, is supported in functions but a maximum recursion depth (of currently 1024 by default) is imposed. The limit can be changed in tremor-server using the --recursion-limit LIMIT argument. As tremor is primarily an event processing engine there are no facilities for user defined logic to loop or recurse infinitely. Recursion is indicated by the recur expression that gets passed data from the current iteration as arguments for the following invocation. Functions may access constants but cannot access external mutable state. Functions are limited to only call functions that were defined prior to themselves, this limits the risk of cyclic recursion between multiple functions and ensure that every call is guaranteed to terminate.","title":"Functions"},{"location":"0010-modularity/#functions-come-in-multiple-forms","text":"","title":"Functions come in multiple forms:"},{"location":"0010-modularity/#intrinsic-functions","text":"Intrinsic functions provide the signature of a builtin function. These are provided for documentation purposes and so that API documentation can be provided for builtin functions in the same way as user defined functions. ### The `test` module is used for writing tremor unit ### tests. ## Runs an assertion for a test, ensures that `expected` and ## `got` are the ## same. If not errors. ## ## **WARNING**: Do not run assertions in production code! ## ## Returns an `bool`. intrinsic fn assert ( name , expected , got ) as test :: assert ;","title":"Intrinsic functions"},{"location":"0010-modularity/#ordinary-functions","text":"Of the form fn <name>([<args>][,...]) with provide named arguments with optional variable arguments through the ellipses ... or varargs operator. Varargs are stored in the args array. The ordinary form does not support partial functions. An ordinary function wrapping a call to a tail recursive fibonacci function: fn fib ( n ) with fib_ ( 0 , 1 , n ) end ; fib ( 7 ); # Call locally defined function fib","title":"Ordinary functions"},{"location":"0010-modularity/#matching-functions","text":"Matching functions using fn <name>(<args>) of followed by case expressions and an optional default statement that match. The matching function form imposes a default case requirement so that unmatches cases have error handling defined. Unlike match expressions the default case in user defined functions must not ( and can not ) be omitted. A contrived example showing math functions with value matching, extractor matching and function case guards. use std :: type ; fn snottify ( s ) of case ( \"badger\" ) => \"snot badger, hell yea!\" case (\\~ json ||) => let s . snot = true , s case ( s ) when type :: is_string ( s ) => \"snot {s}\" default => \"snot caller, you can't snottify that!\" end ;","title":"Matching Functions"},{"location":"0010-modularity/#recursive-functions","text":"Tail recursive functions follow the signature of the function over which recursion is being invoked and use a recur(<args>) call expression. If the signature of a recursive call supports partial function semantics then this is respected under tail recursion. If the signature of a recursive call supports varargs semantics then this is respected under tail recursion. A tail-recursive implementation of fibonacci called by fib(n) above: fn fib_ ( a , b , n ) of case ( a , b , n ) when n > 0 => recur ( b , a + b , n - 1 ) default => a end ;","title":"Recursive Functions"},{"location":"0010-modularity/#limitations-and-constraints","text":"","title":"Limitations and constraints:"},{"location":"0010-modularity/#functions-can-only-be-defined-with-a-singular-arity","text":"Functions currently can not be redefined with multiple arities. So a function foo(n) precludes a second definition of a function called foo with two or more arguments. However the function foo(n,...) defines a function that can take one or more arguments. This constraint may be lifted in the future once usage and adoption favor enhancing functionality.","title":"Functions can only be defined with a singular arity"},{"location":"0010-modularity/#higher-order-functions-are-not-supported","text":"As the type system underpinning tremor-script and tremor-query does not support expression or function references, higher order functions are thus not supported at this time.","title":"Higher Order functions are not supported"},{"location":"0010-modularity/#hardcoded-recursion-depth","text":"Although functions are tail-recursive and stack limits are not a functional concern, the tremor event processing system is primarily designed for event streaming applications. A recursion depth is imposed to prevent functions from recursing indefinitely and blocking event streams from progressing. This is a feature, not a constraint. But it is important to be aware of when developing will behaved functions. At this point in time the maximum depth is 1024 and can not be changed without recompiling tremor.","title":"Hardcoded Recursion Depth"},{"location":"0010-modularity/#tuple-patterns","text":"As a side effect of adding functions this RFC introduces tuple patterns. Internally they are used to implement Matching functions but are available for use in match statements as well. Tuple patterns are written in the form of %(<pattern 1>, <pattern 2>) for patterns with a fixed number of elements or %(<pattern 1>, <pattern 2>, ...) with literal ... for open patterns. Tuple patterns with a fixed number of elements match arrays, with the same number of elements where each element matches the pattern in the same place. So the first pattern must match the first element of the array, the second pattern the second element and so on. An open pattern matches an array that is at lest the same number of elements as the pattern but can have more otherwise the rules are the same. There is also a new pattern introduced to predicate patterns which is the \"I don't care\" pattern _ which will match every element on an array. So: let o = origin :: as_uri_record (); match o of # matches for the path: # \"/api/v1/get/<something>\" # \"/api/v1/get/<something>/snot\" # \"/api/v1/get/<something>/snot/badger\" # and so one case %( \"api\" , \"v1\" , \"get\" , _ , ...) => \"get request\" end ;","title":"Tuple patterns"},{"location":"0010-modularity/#reference-level-explanation","text":"The module path, modules and use rule provide the language and runtime agnostic core facilities that allow queries in tremor-query and code in tremor-script to be namespaced logically via the mod syntax and physically on file systems. A new lexical preprocessing phase parses out occurrences of use rules in scripts, queries and embedded scripts replacing them with preprocessing directives and the contents of referenced modules. The parser in modular scripts and queries must now keep track of relative and absolute module scope. As support for both logical namespaces via the mod syntax and physical isolation through the file system and module path is supported, an external module effectively defines a module namespace. So a source file foo.trickle at the root of the module path would ordinarily be used to include definitions into another query with a use foo; declaration at the head of the file. Nested directories also form namespaces so bar/baz/snot.trickle would be declared for use as use bar::baz::snot . The same layout and rules apply for scripts so bar/baz/snot.trickle would be declared for use as use bar::baz::snot . Where modules of the same base name in the physical file system such as 'badger.tremor' and 'badger.trickle' are present in the same module path behaviour of the runtime is currently undefined. The prototype implementation of modules in tremor that accompanies this RFC gracefully handle the situation for embedded tremor scripts within trickle query files. As a convenience to developers, developer tools such as tremor-tool , tremor-script and tremor-query automatically included the current working directory as a mount point in the module path. If a TREMOR_PATH environment variable is set then it overrides any default behaviour. In tremor-server TREMOR_PATH is required to be set or no include path will be available. This is reflected in the docker image.","title":"Reference-level explanation"},{"location":"0010-modularity/#drawbacks","text":"Modularising logic in tremor increases complexity of the engine and runtime, however the relative increase in complexity is perceived as negligible given the value gained by developers by introducing the facility. At this time higher order functions are not supported as the tremor type system is constrained to JSON compatible value types and introducing module or function references would make the type system asymmetric with JSON. This is left for a future RFC. At this time the query language supports only modularising definitions of windows, operators or scripts. The creation of query language pipeline graph nodes or link of nodes in a graph is not supported in external modules.","title":"Drawbacks"},{"location":"0010-modularity/#rationale-and-alternatives","text":"The design of the module mechanism and its application to tremor-script and tremor-query provide the highest degree of reuse whilst imposing the lowest runtime impact today and without closing off opportunities for evolving and improving the mechanisms in future.","title":"Rationale and alternatives"},{"location":"0010-modularity/#prior-art","text":"This RFC and it's implementation draws inspiration from the C preprocessor as well as the application of use in Rust, and the functional pattern matching style from the Erlang programming language.","title":"Prior art"},{"location":"0010-modularity/#unresolved-questions","text":"None.","title":"Unresolved questions"},{"location":"0010-modularity/#future-possibilities","text":"Currently var arg functions do not combine with either match or recursion or matching functions. This presents a good future opportunity for extending functions. Another future possibility is to expand the capabilities of use in trickle to include full sub queries.","title":"Future possibilities"},{"location":"0011-string-interpolation/","text":"Feature Name: string_interpolation Start Date: 2020-11-30 Tremor Issue: tremor-rs/tremor-runtime#726 RFC PR: tremor-rs/tremor-rfcs#34 Summary \u00b6 This RFC covers the implementation and behavior of string interpolation in both strings and heredocs. Motivation \u00b6 String interpolation is a powerful tool to create text in a dynamic fashion. Getting it to work in a consistent and unsurprising manner however is non-trivial. The goal of this RFC is to cover various edge cases to find an optimal interface. It also will discuss integration with the std::string::format function as it offers a related mechanism. Guide-level explanation \u00b6 Interpolation should work equally in heredocs and strings, for readability we will refer to it as string interpolation in an inclusive way. The goal of string interpolation is making the creation of strings simpler. The basic facility is the ability to embed code inside a string that is executed and it's result then integrated into the string. There are more advanced systems such as templating libraries like mustache , however for now that kind of feature set is outside of the scope of the RFC. For simplicity of use we will include strings and numbers verbatim but render all other data structures as JSON. An alternative approach would be to enforce the type to be primitive or even string but that feels like an unnecessary burden as auto conversion does not prevent manual conversion. Considerations for tremor \u00b6 The syntax we are looking at is using {} to encapsulate data to be interpolated. \"13 + 29 = {13 + 29}\" We are also using {} in string::format which leads to conflicts. An alternative would be to adopt a more verbose form of interpolation and prefix the {...} with an additional character. A few options could be: ${ ... } - the drawback being here is that $ is already used to stand for metadata , by that overloading the sign. This is a well-known format. #{ ... } - # is currently used for comments, by that overloading the sign. This is a well-known format. %{ ... } - the % sign with squiggly is currently also a record pattern for extractors, by that overloading the sign. !{ ... } - the ! sign is easy to miss and not very readable @{ ... } - no overloading at the moment &{ ... } - no overloading at the moment *{ ... } - no overloading at the moment {{ ... }} - no overloading, a bit verbose Use cases \u00b6 string only \u00b6 \"13 + 29 = {13 + 29} escaped: \\{} {{}}\" \"13 + 29 = ${13 + 29} escaped: \\${}\" \"13 + 29 = #{ 13 + 29 } escaped: \\# {}\" \"13 + 29 = %{13 + 29} escaped: \\%{}\" \"13 + 29 = !{13 + 29} escaped: \\!{}\" \"13 + 29 = @{13 + 29} escaped: \\@{}\" \"13 + 29 = {{13 + 29}} escaped: \\{{}}\" object keys \u00b6 { \"key_{13 + 29} escaped: \\{} {{}}\" : 1 , \"key_${13 + 29} escaped: \\${}\" : 2 , \"key_ #{ 13 + 29 } escaped: \\# {}\" : 3 , \"key_%{13 + 29} escaped: \\%{}\" : 4 , \"key_!{13 + 29} escaped: \\!{}\" : 5 , \"key_@{13 + 29} escaped: \\@{}\" : 6 , \"key_{{13 + 29}} escaped: \\{{}}\" : 7 } code generation \u00b6 \"\"\" # using \\ escape \\{ \"key\": \\{ \"key1\": 42 }, \"snot\": {badger}, } # using {{}} escape {{ \"key\": {{ \"key1\": 42 }}, \"snot\": {badger}, }} # using any of the prefix ones { \"key\": { \"key1\": 42 }, \"snot\": #{badger}, } # using double squigly { \"key\": { \"key1\": 42 }, \"snot\": {{badger}}, } \"\"\" Observations on escaping \u00b6 There are competing concerns regarding escaping. In this section those opposing considerations are laid out. One of the desires is reduce complexity and the need to learn new syntax for an operator. This guides us towards a single escape character, \\ is what we use everywhere else, so the conclusion form this desire would be to escape the beginning with a \\ to be in line with all other escapes. A competing concern is the desire of readability, using \\{ to escape doesn't read well and introduces asymmetry unless \\} is also escaped, however then \\} would be escaped without aq reason to do so, and suddenly allow } be written as \\} and } that adds possible style and usage complexity. To address asynchronicity an option is using {{ and }} those read nicely and are symmetric however this introduces an unnessessary escape of }} and leads to the situation where there are two forms of correct code: \"this {{ is not }} interpolated\" as well as \"this {{ is not } interpolated\" . A middle ground is to pick a two-character start sequence for interpolation, this mirrors what most other languages do. Adding a leading character such as # , $ , @ etc means that {} no longer has to be escaped and instead the escape is for that leading character, and only when it is followed by a { . To give an example, using # : \"this #{\"is\"} interpolated\" \"this { is not } interpolated\" or for the more complex case of code generation: \"\"\" { \"key\": { \"key1\": \"and this is \\#{not interpolated}\" }, \"snot\": #{interpolated_variable}, } \"\"\" Interaction with format \u00b6 use case: building a format string dynamically and inputting data \u00b6 let f = match event . type of case \"warning\" => \"be warned, something went wrong: {}\" case \"error\" => \"ERROR OMG! {}\" , default => \"oh, hi, you can ignore this: {}\" end ; string :: format ( format , event . msg ); format and interpolation \u00b6 string :: format ( \"this should work {}\" , 42 ) == \"this should work 42\" ; string :: format ( \"this should {\" also work \"} {}\" , 42 ) == \"this should also work 42\" ; string :: format ( \"this should create a {{}} and say {}\" , 42 ) == \"this should create a {} and say 42\" ; string :: format ( \"this should {\" also work \"}, create a {{}} and show {}\" , 42 ) == \"this should also work, create a {} and say 42\" ; This can cause issues since both interpolation and format give {} a special meaning. There are multiple possible solutions to this. \"Deal with it\" this means that passing an interpolated string into format will require different escaping as a non interpolated string. This adds an unnecessary burden and learning curve on users and should be avoided. Choose a different placeholder for format. This will break backward compatibility it has the advantage that revisiting string::format might make it more powerful. Choose a different for interpolation, such as a prefix. would also resolve the problem and break backward compatibility but allow exploring alternative ways for interpolation. Remove string::format, this would break backward compatibility but simplify things in the long run Option 2 and 3 are non-exclusive Conclusion \u00b6 Between the required tradeoffs to be made, using a two letter starting sequence looks like the sweet spot. upsides: string::format compatibility: as {} no longer has a special meaning, this means string::format needs no change balanced escaping: as { no longer needs to be escaped, neither does } . escaping in code generation: since a two character sequence is significantly less likely to overlap - it will not eliminate the potential need for it, but that is always true As there is only one sequence and one way to write it it keeps the complexity at the same level Since it is the most common approach for interpolation, it is easier to learn by knowledge transfer downsides: it is a breaking change (however every change would be) From the survey of of existing implementations the most common form seems to be ${...} , followed closely by #{...} both seem workable however, both $ and # have their own meaning in tremor script. # is the comment character, so it can be reasond that it is switching contexts from code to comment, or from string to interpolation. $ is our metadata , which could be reasond that ${...} is a meta context, however this seems to be a bit of a stretch and ${$} would read quite odd. Considering all this, the proposed solution for this RFC is to use #{...} for string interpolation. Reference-level explanation \u00b6 The implementation holds a few requirements: non interpolated strings should have no additional overhead. constant folding should apply to interpolation. interpolated strings should, at worst, have the same cost as string concatenation, at best be more preformat. errors should remain hygienic and useful. the behavior of interpolation should be the same in strings and heredocs aside of the interpolation, behavior of interpolated and non interpolated strings should remain the same (read: adding an interpolated item to a string should not change behavior) Drawbacks \u00b6 There are some drawbacks to consider. From a user perspective string interpolation adds to the cognitive complexity as it introduces a new logical element and new escaping requirements, this can partially be mitigated with good integration in highlighting to ease the readability. Another drawback to consider is hidden performance cost, a string, intuitively, feels cheep as it is a primitive datatype, by adding interpolation strings become significantly more 'expensive' without being very explicit about it. The overlap with string::format introduces the situation where we have two mechanisms that to the most degree solve the same problem. Giving users this choice makes it harder to write (and by that read) 'idiomatic' tremor code. The choice of syntax comes with different drawbacks. Using a single {} style will force additional escaping, a longer syntax, for example one of the prefixed ones, will be more typing but would resolve a lot of escaping needs as *{ is going to be a lot less common then a single { . Rationale and alternatives \u00b6 String interpolation feels more natural then using format functions or concatenation. While the aforementioned drawbacks exist they are outweighed by the gain in usability. An alternative would be to extend the format function and drop interpolation, but the general aim for operator friendliness and usability rules this out. Prior art \u00b6 There are many existing implementations of string interpolation, for reference here are a few each producing the string 13 + 29 = 42 : two character sequences \u00b6 Ruby \"13 + 29 = #{ 13 + 29 } Elixir \"13 + 29 = #{ 13 + 29 } Coffeescript \"13 + 12 = #{ 13 + 29 } \" Javascript `13 + 29 = ${ 13 + 29 } ` Groovy (also supports variable access via $) \"13 + 12 = ${13 + 29}\" Haxe (also supports variable access via $) \"13 + 12 = ${13 + 29}\" Dart (also supports variable access via $) \"13 + 12 = ${ 13 + 29 } \" BASH \"13 + 12 = $( expr 13 + 29 ) \" LUA \"13 + 12 = %(13 + 29)\" SWIFT \"13 + 12 = \\( 13 + 29 ) \" Julia (also supports variable access via $) \"13 + 12 = $ ( 13 + 29 ) \" second string type \u00b6 C# $\"13 + 12 = {13 + 12}\" Kotlin (also supports variable access via $) var answer = ; \"13 + 29 = ${ 13 + 29 } \" second string type \u00b6 F# $ \"13 + 12 = {13 + 12}\" Python f '13 + 29 = { 13 + 29 } ' Kotlin (limited to variables) val answer = 13 + 29 ; s \"13 + 29 = $ answer \" Haskell (via library) [ i | 13 + 12 = # { 13 + 29 } | ] Other \u00b6 Perl (limited to variables) answer = 13 + 29 ; \"13 + 29 = $answer\" PHP (limited to variables) $answer = 13 + 29; \"13 + 29 = $answer\" A more extensive list including replacement and concatination can be found on rosettacode Unresolved questions \u00b6 Is it worth sticking with {} for interpolation despite the drawbacks on escaping What longer sequence would be the most appropriate Future possibilities \u00b6 This RFC does exclude considerations for a templateing language. This is however a possible extension for the future worth keeping in mind, but might require a separate file format or different syntax to keep the complexity at bay.","title":"0011-string-interpolation"},{"location":"0011-string-interpolation/#summary","text":"This RFC covers the implementation and behavior of string interpolation in both strings and heredocs.","title":"Summary"},{"location":"0011-string-interpolation/#motivation","text":"String interpolation is a powerful tool to create text in a dynamic fashion. Getting it to work in a consistent and unsurprising manner however is non-trivial. The goal of this RFC is to cover various edge cases to find an optimal interface. It also will discuss integration with the std::string::format function as it offers a related mechanism.","title":"Motivation"},{"location":"0011-string-interpolation/#guide-level-explanation","text":"Interpolation should work equally in heredocs and strings, for readability we will refer to it as string interpolation in an inclusive way. The goal of string interpolation is making the creation of strings simpler. The basic facility is the ability to embed code inside a string that is executed and it's result then integrated into the string. There are more advanced systems such as templating libraries like mustache , however for now that kind of feature set is outside of the scope of the RFC. For simplicity of use we will include strings and numbers verbatim but render all other data structures as JSON. An alternative approach would be to enforce the type to be primitive or even string but that feels like an unnecessary burden as auto conversion does not prevent manual conversion.","title":"Guide-level explanation"},{"location":"0011-string-interpolation/#considerations-for-tremor","text":"The syntax we are looking at is using {} to encapsulate data to be interpolated. \"13 + 29 = {13 + 29}\" We are also using {} in string::format which leads to conflicts. An alternative would be to adopt a more verbose form of interpolation and prefix the {...} with an additional character. A few options could be: ${ ... } - the drawback being here is that $ is already used to stand for metadata , by that overloading the sign. This is a well-known format. #{ ... } - # is currently used for comments, by that overloading the sign. This is a well-known format. %{ ... } - the % sign with squiggly is currently also a record pattern for extractors, by that overloading the sign. !{ ... } - the ! sign is easy to miss and not very readable @{ ... } - no overloading at the moment &{ ... } - no overloading at the moment *{ ... } - no overloading at the moment {{ ... }} - no overloading, a bit verbose","title":"Considerations for tremor"},{"location":"0011-string-interpolation/#use-cases","text":"","title":"Use cases"},{"location":"0011-string-interpolation/#string-only","text":"\"13 + 29 = {13 + 29} escaped: \\{} {{}}\" \"13 + 29 = ${13 + 29} escaped: \\${}\" \"13 + 29 = #{ 13 + 29 } escaped: \\# {}\" \"13 + 29 = %{13 + 29} escaped: \\%{}\" \"13 + 29 = !{13 + 29} escaped: \\!{}\" \"13 + 29 = @{13 + 29} escaped: \\@{}\" \"13 + 29 = {{13 + 29}} escaped: \\{{}}\"","title":"string only"},{"location":"0011-string-interpolation/#object-keys","text":"{ \"key_{13 + 29} escaped: \\{} {{}}\" : 1 , \"key_${13 + 29} escaped: \\${}\" : 2 , \"key_ #{ 13 + 29 } escaped: \\# {}\" : 3 , \"key_%{13 + 29} escaped: \\%{}\" : 4 , \"key_!{13 + 29} escaped: \\!{}\" : 5 , \"key_@{13 + 29} escaped: \\@{}\" : 6 , \"key_{{13 + 29}} escaped: \\{{}}\" : 7 }","title":"object keys"},{"location":"0011-string-interpolation/#code-generation","text":"\"\"\" # using \\ escape \\{ \"key\": \\{ \"key1\": 42 }, \"snot\": {badger}, } # using {{}} escape {{ \"key\": {{ \"key1\": 42 }}, \"snot\": {badger}, }} # using any of the prefix ones { \"key\": { \"key1\": 42 }, \"snot\": #{badger}, } # using double squigly { \"key\": { \"key1\": 42 }, \"snot\": {{badger}}, } \"\"\"","title":"code generation"},{"location":"0011-string-interpolation/#observations-on-escaping","text":"There are competing concerns regarding escaping. In this section those opposing considerations are laid out. One of the desires is reduce complexity and the need to learn new syntax for an operator. This guides us towards a single escape character, \\ is what we use everywhere else, so the conclusion form this desire would be to escape the beginning with a \\ to be in line with all other escapes. A competing concern is the desire of readability, using \\{ to escape doesn't read well and introduces asymmetry unless \\} is also escaped, however then \\} would be escaped without aq reason to do so, and suddenly allow } be written as \\} and } that adds possible style and usage complexity. To address asynchronicity an option is using {{ and }} those read nicely and are symmetric however this introduces an unnessessary escape of }} and leads to the situation where there are two forms of correct code: \"this {{ is not }} interpolated\" as well as \"this {{ is not } interpolated\" . A middle ground is to pick a two-character start sequence for interpolation, this mirrors what most other languages do. Adding a leading character such as # , $ , @ etc means that {} no longer has to be escaped and instead the escape is for that leading character, and only when it is followed by a { . To give an example, using # : \"this #{\"is\"} interpolated\" \"this { is not } interpolated\" or for the more complex case of code generation: \"\"\" { \"key\": { \"key1\": \"and this is \\#{not interpolated}\" }, \"snot\": #{interpolated_variable}, } \"\"\"","title":"Observations on escaping"},{"location":"0011-string-interpolation/#interaction-with-format","text":"","title":"Interaction with format"},{"location":"0011-string-interpolation/#use-case-building-a-format-string-dynamically-and-inputting-data","text":"let f = match event . type of case \"warning\" => \"be warned, something went wrong: {}\" case \"error\" => \"ERROR OMG! {}\" , default => \"oh, hi, you can ignore this: {}\" end ; string :: format ( format , event . msg );","title":"use case: building a format string dynamically and inputting data"},{"location":"0011-string-interpolation/#format-and-interpolation","text":"string :: format ( \"this should work {}\" , 42 ) == \"this should work 42\" ; string :: format ( \"this should {\" also work \"} {}\" , 42 ) == \"this should also work 42\" ; string :: format ( \"this should create a {{}} and say {}\" , 42 ) == \"this should create a {} and say 42\" ; string :: format ( \"this should {\" also work \"}, create a {{}} and show {}\" , 42 ) == \"this should also work, create a {} and say 42\" ; This can cause issues since both interpolation and format give {} a special meaning. There are multiple possible solutions to this. \"Deal with it\" this means that passing an interpolated string into format will require different escaping as a non interpolated string. This adds an unnecessary burden and learning curve on users and should be avoided. Choose a different placeholder for format. This will break backward compatibility it has the advantage that revisiting string::format might make it more powerful. Choose a different for interpolation, such as a prefix. would also resolve the problem and break backward compatibility but allow exploring alternative ways for interpolation. Remove string::format, this would break backward compatibility but simplify things in the long run Option 2 and 3 are non-exclusive","title":"format and interpolation"},{"location":"0011-string-interpolation/#conclusion","text":"Between the required tradeoffs to be made, using a two letter starting sequence looks like the sweet spot. upsides: string::format compatibility: as {} no longer has a special meaning, this means string::format needs no change balanced escaping: as { no longer needs to be escaped, neither does } . escaping in code generation: since a two character sequence is significantly less likely to overlap - it will not eliminate the potential need for it, but that is always true As there is only one sequence and one way to write it it keeps the complexity at the same level Since it is the most common approach for interpolation, it is easier to learn by knowledge transfer downsides: it is a breaking change (however every change would be) From the survey of of existing implementations the most common form seems to be ${...} , followed closely by #{...} both seem workable however, both $ and # have their own meaning in tremor script. # is the comment character, so it can be reasond that it is switching contexts from code to comment, or from string to interpolation. $ is our metadata , which could be reasond that ${...} is a meta context, however this seems to be a bit of a stretch and ${$} would read quite odd. Considering all this, the proposed solution for this RFC is to use #{...} for string interpolation.","title":"Conclusion"},{"location":"0011-string-interpolation/#reference-level-explanation","text":"The implementation holds a few requirements: non interpolated strings should have no additional overhead. constant folding should apply to interpolation. interpolated strings should, at worst, have the same cost as string concatenation, at best be more preformat. errors should remain hygienic and useful. the behavior of interpolation should be the same in strings and heredocs aside of the interpolation, behavior of interpolated and non interpolated strings should remain the same (read: adding an interpolated item to a string should not change behavior)","title":"Reference-level explanation"},{"location":"0011-string-interpolation/#drawbacks","text":"There are some drawbacks to consider. From a user perspective string interpolation adds to the cognitive complexity as it introduces a new logical element and new escaping requirements, this can partially be mitigated with good integration in highlighting to ease the readability. Another drawback to consider is hidden performance cost, a string, intuitively, feels cheep as it is a primitive datatype, by adding interpolation strings become significantly more 'expensive' without being very explicit about it. The overlap with string::format introduces the situation where we have two mechanisms that to the most degree solve the same problem. Giving users this choice makes it harder to write (and by that read) 'idiomatic' tremor code. The choice of syntax comes with different drawbacks. Using a single {} style will force additional escaping, a longer syntax, for example one of the prefixed ones, will be more typing but would resolve a lot of escaping needs as *{ is going to be a lot less common then a single { .","title":"Drawbacks"},{"location":"0011-string-interpolation/#rationale-and-alternatives","text":"String interpolation feels more natural then using format functions or concatenation. While the aforementioned drawbacks exist they are outweighed by the gain in usability. An alternative would be to extend the format function and drop interpolation, but the general aim for operator friendliness and usability rules this out.","title":"Rationale and alternatives"},{"location":"0011-string-interpolation/#prior-art","text":"There are many existing implementations of string interpolation, for reference here are a few each producing the string 13 + 29 = 42 :","title":"Prior art"},{"location":"0011-string-interpolation/#two-character-sequences","text":"Ruby \"13 + 29 = #{ 13 + 29 } Elixir \"13 + 29 = #{ 13 + 29 } Coffeescript \"13 + 12 = #{ 13 + 29 } \" Javascript `13 + 29 = ${ 13 + 29 } ` Groovy (also supports variable access via $) \"13 + 12 = ${13 + 29}\" Haxe (also supports variable access via $) \"13 + 12 = ${13 + 29}\" Dart (also supports variable access via $) \"13 + 12 = ${ 13 + 29 } \" BASH \"13 + 12 = $( expr 13 + 29 ) \" LUA \"13 + 12 = %(13 + 29)\" SWIFT \"13 + 12 = \\( 13 + 29 ) \" Julia (also supports variable access via $) \"13 + 12 = $ ( 13 + 29 ) \"","title":"two character sequences"},{"location":"0011-string-interpolation/#second-string-type","text":"C# $\"13 + 12 = {13 + 12}\" Kotlin (also supports variable access via $) var answer = ; \"13 + 29 = ${ 13 + 29 } \"","title":"second string type"},{"location":"0011-string-interpolation/#second-string-type_1","text":"F# $ \"13 + 12 = {13 + 12}\" Python f '13 + 29 = { 13 + 29 } ' Kotlin (limited to variables) val answer = 13 + 29 ; s \"13 + 29 = $ answer \" Haskell (via library) [ i | 13 + 12 = # { 13 + 29 } | ]","title":"second string type"},{"location":"0011-string-interpolation/#other","text":"Perl (limited to variables) answer = 13 + 29 ; \"13 + 29 = $answer\" PHP (limited to variables) $answer = 13 + 29; \"13 + 29 = $answer\" A more extensive list including replacement and concatination can be found on rosettacode","title":"Other"},{"location":"0011-string-interpolation/#unresolved-questions","text":"Is it worth sticking with {} for interpolation despite the drawbacks on escaping What longer sequence would be the most appropriate","title":"Unresolved questions"},{"location":"0011-string-interpolation/#future-possibilities","text":"This RFC does exclude considerations for a templateing language. This is however a possible extension for the future worth keeping in mind, but might require a separate file format or different syntax to keep the complexity at bay.","title":"Future possibilities"},{"location":"0012-correlation/","text":"Feature Name: correlation_linked_transport Start Date: 2021-03-12 Tremor Issue: tremor-rs/tremor-runtime#0000 RFC PR: tremor-rs/tremor-rfcs#0000 Summary \u00b6 Linked Transports enable integrating request-response based communications with tremor event-streams. Events coming from pipelines can be turned into requests and responses are turned into events and sent on to other pipelines. Nonetheless we currently can't correlate them, that is have request event data present in the context of the response event handling. This RFC is suggesting new means for convenient correlating of events, i.e. the special $correlation metadata key. Motivation \u00b6 When we introduced Linked Transport we enabled request-response communication patterns with the outside world, like HTTP or some Websocket protocols. Right now request and response handling need to either be done in two different pipelines or in the same and be dispatched within the trickle/tremor-script logic. If any part of the request event needs to be around for response handling, the only option is to handle both in 1 pipeline and use the state mechanism to store and retrieve the request event data upon response handling. We are currently adding some correlation data e.g. for the elastic offramp, where we store the whole origin event_id , origin_uri and the payload of the document indexed. Or the rest offramp, where we include the HTTP request metadata into the response event. Those are in no means complete in that we can only correlate metadata that is also being sent to as the event itself (HTTP headers, elastic document payload). We want the correlation mechanism to be more flexible and to not affect the actual event payload or outgoing protocol unit. Correlation should be an internal mechanism to your tremor application logic. Guide-level explanation \u00b6 Every Linked Transport onramp or offramp will for every incoming event take the Value at the metadata key $correlation , if any and inject it into the response event metadata under the same key. This way users can pass correlation data from request to response without the need to manipulate the event payload and thus the application data to be sent out. Also we only need to keep as much correlation data around as we have in-flight events and most of all we don't require users to write complex and error-prone correlation logic in tremor-script, which will blow up code bases, possibly beyond reasonable maintainability. Usage example using the rest offramp: Here we have the request handling pipeline, that moves some event field into the special $correlation metadata field. # request handling define script extract_correlation_id script # extract application key and put it into correlation let $ correlation = $ request . headers [ \"X-Application-Key\" ] ; event end ; select event from in into extract_correlation_id ; select event from extract_correlation_id / out into out ; select event from extract_correlation_id / err into err ; This is the response handling pipeline which uses the $correlation metadata field in further event processing: # response handling define script correlation script # preparation for sending this response further down the road via another rest offramp let $ request . headers [ \"X-Application-Key\" ] = $ correlation ; let $ request . endpoint = \"http://example.org/application\" ; event end ; select event from in into correlation ; select event from correlation / out into out ; select event from correlation / err into err ; For cases where the event payload should remain unaffected Reference-level explanation \u00b6 As for the implementation, every linked transport needs to be touched and needs to keep around the correlation Value s for each in-flight event and inject it into the response event. This also includes cleaning up the correlation state in case of errors of the external systems, in case of timeouts and the like, so we ensure that we never grow beyond the bounds of the configured concurrency. We need to take care that no offramp/onramp uses that field for its own metadata. Drawbacks \u00b6 The state we need to keep at the Linked Transport offramp/onramp will grow with the supported concurrency (in-flight requests). Implementing this will further complicate the already quite complex Linked Transport implementations. Maybe we should consider implementing this as part of the Connectors RFC . Rationale and alternatives \u00b6 In the initial ideas for this RFC we came up with a correlate operator for \"joining\" a number of events based on some expression, say $correlation . This was equipped with a timeout , so we take care to not keep to much state around and put a limit to the correlation window. I decided against coding this as an operator as it can be implemented with core language features of tremor-query , with a size based tumbling window and a group_by: define tumbling window size_2 with size = 2, eviction_period = 1000 end; select aggr::win::collect_flattened(event) from in[size_2] group by $correlation into out; To get exactly the same timeout behaviour the operator would have, we might need to tweak the current eviction_period handling logic, as it currently only gets rid of groups after 2 x eviction_period . But as the operator can only live in 1 pipeline, we need to pass both events through the same pipeline anyways, and the above code is much more idiomatic and feels more native and less cumbersome. This pattern for correlation will find its way into the docs. Prior art \u00b6 Correlating events is a key mechanism for enabling event tracing, which is usually achieved by adding trace ids to events. This is how e.g. zipkin tracing works. So there is a whole ecosystem around enabling observability with proper tracing that relies on those ids being present and be delivered and maintained across boundaries. While most applications will already include a trace id like this in the event payload, it might still be required to enable keeping those ids while accessing external services via a linked transport. So this enables better tracing scenarios for Tremor. Unresolved questions \u00b6 None. Future possibilities \u00b6 One road to take this RFC idea down is to bake the correlation and event tracing mechanism even deeper into the runtime. What we have now with the internal EventId is some form of limited tracing in that it tracks the minimum and maximum of event ids per source. We could extend this to build causal tracing chains. This is necessary, as at places like a windowed select query, a generic::batch operator we emit new events, with a new id. They do keep track of the events that make up the current one, but we cannot properly trace every single event with this mechanism. Such a tracing chain would be usable for the correlation feature, if made accessible to user pipeline code.","title":"0012-correlation"},{"location":"0012-correlation/#summary","text":"Linked Transports enable integrating request-response based communications with tremor event-streams. Events coming from pipelines can be turned into requests and responses are turned into events and sent on to other pipelines. Nonetheless we currently can't correlate them, that is have request event data present in the context of the response event handling. This RFC is suggesting new means for convenient correlating of events, i.e. the special $correlation metadata key.","title":"Summary"},{"location":"0012-correlation/#motivation","text":"When we introduced Linked Transport we enabled request-response communication patterns with the outside world, like HTTP or some Websocket protocols. Right now request and response handling need to either be done in two different pipelines or in the same and be dispatched within the trickle/tremor-script logic. If any part of the request event needs to be around for response handling, the only option is to handle both in 1 pipeline and use the state mechanism to store and retrieve the request event data upon response handling. We are currently adding some correlation data e.g. for the elastic offramp, where we store the whole origin event_id , origin_uri and the payload of the document indexed. Or the rest offramp, where we include the HTTP request metadata into the response event. Those are in no means complete in that we can only correlate metadata that is also being sent to as the event itself (HTTP headers, elastic document payload). We want the correlation mechanism to be more flexible and to not affect the actual event payload or outgoing protocol unit. Correlation should be an internal mechanism to your tremor application logic.","title":"Motivation"},{"location":"0012-correlation/#guide-level-explanation","text":"Every Linked Transport onramp or offramp will for every incoming event take the Value at the metadata key $correlation , if any and inject it into the response event metadata under the same key. This way users can pass correlation data from request to response without the need to manipulate the event payload and thus the application data to be sent out. Also we only need to keep as much correlation data around as we have in-flight events and most of all we don't require users to write complex and error-prone correlation logic in tremor-script, which will blow up code bases, possibly beyond reasonable maintainability. Usage example using the rest offramp: Here we have the request handling pipeline, that moves some event field into the special $correlation metadata field. # request handling define script extract_correlation_id script # extract application key and put it into correlation let $ correlation = $ request . headers [ \"X-Application-Key\" ] ; event end ; select event from in into extract_correlation_id ; select event from extract_correlation_id / out into out ; select event from extract_correlation_id / err into err ; This is the response handling pipeline which uses the $correlation metadata field in further event processing: # response handling define script correlation script # preparation for sending this response further down the road via another rest offramp let $ request . headers [ \"X-Application-Key\" ] = $ correlation ; let $ request . endpoint = \"http://example.org/application\" ; event end ; select event from in into correlation ; select event from correlation / out into out ; select event from correlation / err into err ; For cases where the event payload should remain unaffected","title":"Guide-level explanation"},{"location":"0012-correlation/#reference-level-explanation","text":"As for the implementation, every linked transport needs to be touched and needs to keep around the correlation Value s for each in-flight event and inject it into the response event. This also includes cleaning up the correlation state in case of errors of the external systems, in case of timeouts and the like, so we ensure that we never grow beyond the bounds of the configured concurrency. We need to take care that no offramp/onramp uses that field for its own metadata.","title":"Reference-level explanation"},{"location":"0012-correlation/#drawbacks","text":"The state we need to keep at the Linked Transport offramp/onramp will grow with the supported concurrency (in-flight requests). Implementing this will further complicate the already quite complex Linked Transport implementations. Maybe we should consider implementing this as part of the Connectors RFC .","title":"Drawbacks"},{"location":"0012-correlation/#rationale-and-alternatives","text":"In the initial ideas for this RFC we came up with a correlate operator for \"joining\" a number of events based on some expression, say $correlation . This was equipped with a timeout , so we take care to not keep to much state around and put a limit to the correlation window. I decided against coding this as an operator as it can be implemented with core language features of tremor-query , with a size based tumbling window and a group_by: define tumbling window size_2 with size = 2, eviction_period = 1000 end; select aggr::win::collect_flattened(event) from in[size_2] group by $correlation into out; To get exactly the same timeout behaviour the operator would have, we might need to tweak the current eviction_period handling logic, as it currently only gets rid of groups after 2 x eviction_period . But as the operator can only live in 1 pipeline, we need to pass both events through the same pipeline anyways, and the above code is much more idiomatic and feels more native and less cumbersome. This pattern for correlation will find its way into the docs.","title":"Rationale and alternatives"},{"location":"0012-correlation/#prior-art","text":"Correlating events is a key mechanism for enabling event tracing, which is usually achieved by adding trace ids to events. This is how e.g. zipkin tracing works. So there is a whole ecosystem around enabling observability with proper tracing that relies on those ids being present and be delivered and maintained across boundaries. While most applications will already include a trace id like this in the event payload, it might still be required to enable keeping those ids while accessing external services via a linked transport. So this enables better tracing scenarios for Tremor.","title":"Prior art"},{"location":"0012-correlation/#unresolved-questions","text":"None.","title":"Unresolved questions"},{"location":"0012-correlation/#future-possibilities","text":"One road to take this RFC idea down is to bake the correlation and event tracing mechanism even deeper into the runtime. What we have now with the internal EventId is some form of limited tracing in that it tracks the minimum and maximum of event ids per source. We could extend this to build causal tracing chains. This is necessary, as at places like a windowed select query, a generic::batch operator we emit new events, with a new id. They do keep track of the events that make up the current one, but we cannot properly trace every single event with this mechanism. Such a tracing chain would be usable for the correlation feature, if made accessible to user pipeline code.","title":"Future possibilities"},{"location":"0013-expr-based-path/","text":"Feature Name: Expression based path root Start Date: 2021-08-01 Tremor Issue: tremor-rs/tremor-runtime#1165 RFC PR: tremor-rs/tremor-rfcs#54 Summary \u00b6 This RFC suggests extending that path to allow additional root elements, namely (immutable) expressions. This includes function calls, record, and list (semi)literals and complex expressions enclosed in () . Motivation \u00b6 It is sometimes cumbersome to store a result in an intermediate value to extract only a single sub-element from the result. It is also counterintuitive not to use an expression such as function(arg).something and can lead to unexpected syntax errors from tremor. Guide-level explanation \u00b6 No new concepts are introduced. The existing concept of a path is extended. As of today, a path could originate from: event an event $ the event metadata a local variable a constant the build-in keywords args , group and state We suggest extending this with four more variants: {...} record literals [...] array literals struct::keys(a_record) a function call or a the return for it (<immutable expr>) any immutable expression in paratheses Reference-level explanation \u00b6 This is realized by either: using the returned reference from the expression as a root for a lookup introducing a new temporary local variable to assign the returned owned variable to and then using a reference to this as a root for the lookup Drawbacks \u00b6 The only drawback discovered during this PR is that it introduces a possibility to create less readable scripts where it would have made sense to introduce a named variable for readability, but instead, an expression root was used. For example the first can be more readable then the second especially if the match statement grows: let key = match event of case % { present k1} => event . k1 case % { present k2} => event . k2 case % { present k3} => event . k3 case _ => event end ; key . badger ## or: ( match event of case % { present k1} => event . k1 case % { present k2} => event . k2 case % { present k3} => event . k3 case _ => event end ). badger Rationale and alternatives \u00b6 An alternative would be leaving it as it is, but in many cases, this is counterintuitive. For example, struct::keys(event)[0] for \"get me the first event key\" feels natural while let keys = struct::keys(event); keys[0] does not. Prior art \u00b6 The current expression syntax is realistically the prior art this lead to, along with many other languages that allow path expressions on functions, expressions, or literals. Unresolved questions \u00b6 Should we allow any expression as a root? Right now, we purposefully exclude mutable expressions to prevent complexity from creeping into scripts. Still, there also is an argument to be made to allow any expression as a root since tremor-script is ultimately an expression-oriented language. Future possibilities \u00b6 Extend this to other expressions as root elements. Include constant folding for looking up keys in constant roots.","title":"0013-expr-based-path"},{"location":"0013-expr-based-path/#summary","text":"This RFC suggests extending that path to allow additional root elements, namely (immutable) expressions. This includes function calls, record, and list (semi)literals and complex expressions enclosed in () .","title":"Summary"},{"location":"0013-expr-based-path/#motivation","text":"It is sometimes cumbersome to store a result in an intermediate value to extract only a single sub-element from the result. It is also counterintuitive not to use an expression such as function(arg).something and can lead to unexpected syntax errors from tremor.","title":"Motivation"},{"location":"0013-expr-based-path/#guide-level-explanation","text":"No new concepts are introduced. The existing concept of a path is extended. As of today, a path could originate from: event an event $ the event metadata a local variable a constant the build-in keywords args , group and state We suggest extending this with four more variants: {...} record literals [...] array literals struct::keys(a_record) a function call or a the return for it (<immutable expr>) any immutable expression in paratheses","title":"Guide-level explanation"},{"location":"0013-expr-based-path/#reference-level-explanation","text":"This is realized by either: using the returned reference from the expression as a root for a lookup introducing a new temporary local variable to assign the returned owned variable to and then using a reference to this as a root for the lookup","title":"Reference-level explanation"},{"location":"0013-expr-based-path/#drawbacks","text":"The only drawback discovered during this PR is that it introduces a possibility to create less readable scripts where it would have made sense to introduce a named variable for readability, but instead, an expression root was used. For example the first can be more readable then the second especially if the match statement grows: let key = match event of case % { present k1} => event . k1 case % { present k2} => event . k2 case % { present k3} => event . k3 case _ => event end ; key . badger ## or: ( match event of case % { present k1} => event . k1 case % { present k2} => event . k2 case % { present k3} => event . k3 case _ => event end ). badger","title":"Drawbacks"},{"location":"0013-expr-based-path/#rationale-and-alternatives","text":"An alternative would be leaving it as it is, but in many cases, this is counterintuitive. For example, struct::keys(event)[0] for \"get me the first event key\" feels natural while let keys = struct::keys(event); keys[0] does not.","title":"Rationale and alternatives"},{"location":"0013-expr-based-path/#prior-art","text":"The current expression syntax is realistically the prior art this lead to, along with many other languages that allow path expressions on functions, expressions, or literals.","title":"Prior art"},{"location":"0013-expr-based-path/#unresolved-questions","text":"Should we allow any expression as a root? Right now, we purposefully exclude mutable expressions to prevent complexity from creeping into scripts. Still, there also is an argument to be made to allow any expression as a root since tremor-script is ultimately an expression-oriented language.","title":"Unresolved questions"},{"location":"0013-expr-based-path/#future-possibilities","text":"Extend this to other expressions as root elements. Include constant folding for looking up keys in constant roots.","title":"Future possibilities"},{"location":"api_changes/","text":"RFC policy - API design \u00b6 Pretty much every change to the API needs an RFC. Note that changes to documentation are not considered changes to API design unless there are associated changes to data structures transmitted or there is a change to error handling or defined behaviour that impacts existing API surface. Deprecations, removal of API methods, and the addition of new methods are considered design changes and should follow semantic versioning principles. API RFCs are managed by the api sub-team, and tagged api . The API sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc. Amendments \u00b6 Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"API Changes"},{"location":"api_changes/#rfc-policy-api-design","text":"Pretty much every change to the API needs an RFC. Note that changes to documentation are not considered changes to API design unless there are associated changes to data structures transmitted or there is a change to error handling or defined behaviour that impacts existing API surface. Deprecations, removal of API methods, and the addition of new methods are considered design changes and should follow semantic versioning principles. API RFCs are managed by the api sub-team, and tagged api . The API sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc.","title":"RFC policy - API design"},{"location":"api_changes/#amendments","text":"Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"Amendments"},{"location":"arch_changes/","text":"RFC policy - architecture design \u00b6 Pretty much every change to the tremor internals architecture needs an RFC. Note that new facilities (or major changes to an existing facilities) are considered changes to tremor architecture. Architecture RFCs are managed by the architecture sub-team, and tagged arch . The architecture sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc. As changes to tremor architecture may intersect with multiple sub-teams, it may require multiple shepherds - one from each sub-team, and a core member to coordinate. In general, changes to core architecture and internals implies a significant investment by the contributor to Tremor and implies that the contributor wishes to become a member committed to continued investment in the project. The core sub-team may wish to discuss commitment with significant contributions to insure progressing those RFCs and long term maintenance and evolution of contributed work. Amendments \u00b6 Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"Architectural Changes"},{"location":"arch_changes/#rfc-policy-architecture-design","text":"Pretty much every change to the tremor internals architecture needs an RFC. Note that new facilities (or major changes to an existing facilities) are considered changes to tremor architecture. Architecture RFCs are managed by the architecture sub-team, and tagged arch . The architecture sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc. As changes to tremor architecture may intersect with multiple sub-teams, it may require multiple shepherds - one from each sub-team, and a core member to coordinate. In general, changes to core architecture and internals implies a significant investment by the contributor to Tremor and implies that the contributor wishes to become a member committed to continued investment in the project. The core sub-team may wish to discuss commitment with significant contributions to insure progressing those RFCs and long term maintenance and evolution of contributed work.","title":"RFC policy - architecture design"},{"location":"arch_changes/#amendments","text":"Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"Amendments"},{"location":"lang_changes/","text":"RFC policy - language design \u00b6 Pretty much every change to the tremor language(s) needs an RFC. Note that new lints (or major changes to an existing lint) are considered changes to the language. Language RFCs are managed by the language sub-team, and tagged lang . The language sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc. Amendments \u00b6 Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"Language Changes"},{"location":"lang_changes/#rfc-policy-language-design","text":"Pretty much every change to the tremor language(s) needs an RFC. Note that new lints (or major changes to an existing lint) are considered changes to the language. Language RFCs are managed by the language sub-team, and tagged lang . The language sub-team will do an initial triage of new PRs within a week of submission. The result of triage will either be that the PR is assigned to a member of the sub-team for shepherding, the PR is closed as postponed because the subteam believe it might be a good idea, but is not currently aligned with Tremor's priorities, or the PR is closed because the sub-team feel it should clearly not be done and further discussion is not necessary. In the latter two cases, the sub-team will give a detailed explanation. We'll follow the standard procedure for shepherding, final comment period, etc.","title":"RFC policy - language design"},{"location":"lang_changes/#amendments","text":"Sometimes in the implementation of an RFC, changes are required. In general these don't require an RFC as long as they are very minor and in the spirit of the accepted RFC (essentially bug fixes). In this case implementers should submit an RFC PR which amends the accepted RFC with the new details. Although the RFC repository is not intended as a reference manual, it is preferred that RFCs do reflect what was actually implemented. Amendment RFCs will go through the same process as regular RFCs, but should be less controversial and thus should move more quickly. When a change is more dramatic, it is better to create a new RFC. The RFC should be standalone and reference the original, rather than modifying the existing RFC. You should add a comment to the original RFC with referencing the new RFC as part of the PR. Obviously there is some scope for judgment here. As a guideline, if a change affects more than one part of the RFC (i.e., is a non-local change), affects the applicability of the RFC to its motivating use cases, or there are multiple possible new solutions, then the feature is probably not 'minor' and should get a new RFC.","title":"Amendments"},{"location":"libs_changes/","text":"RFC guidelines - libraries sub-team \u00b6 Motivation \u00b6 RFCs are heavyweight: RFCs generally take at minimum 2 weeks from posting to land. In practice it can be more on the order of months for particularly controversial changes. RFCs are a lot of effort to write; especially for non-native speakers or for members of the community whose strengths are more technical than literary. RFCs may involve pre-RFCs and several rewrites to accommodate feedback. RFCs require a dedicated shepherd to herd the community and author towards consensus. RFCs require review from a majority of the subteam, as well as an official vote. RFCs can't be downgraded based on their complexity. Full process always applies. Easy RFCs may certainly land faster, though. RFCs can be very abstract and hard to grok the consequences of (no implementation). PRs are low overhead but potentially expensive nonetheless: Easy PRs can get insta-merged by any tremor libraries sub-team contributor. Harder PRs can be easily escalated. You can ping subject-matter experts for second opinions. Ping the whole team! Easier to grok the full consequences. Lots of tests and Crater to save the day. PRs can be accepted optimistically with bors, buildbot, and the trains to guard us from major mistakes making it into stable. The size of the nightly community at this point in time can still mean major community breakage regardless of trains, however. HOWEVER: Big PRs can be a lot of work to make only to have that work rejected for details that could have been hashed out first. RFCs are only meaningful if a significant and diverse portion of the community actively participates in them. The official teams are not sufficiently diverse to establish meaningful community consensus by agreeing amongst themselves. If there are tons of RFCs -- especially trivial ones -- people are less likely to engage with them. Official team members are super busy. Domain experts and industry professionals are super busy and have no responsibility to engage in RFCs. Since these are exactly the most important people to get involved in the RFC process, it is important that we be maximally friendly towards their needs. Is an RFC required? \u00b6 The overarching philosophy is: do whatever is easiest . If an RFC would be less work than an implementation, that's a good sign that an RFC is necessary. That said, if you anticipate controversy, you might want to short-circuit straight to an RFC. Submit a PR if the change is a: Bugfix Docfix Perffixes (with no further impact) Submit an RFC if the change is a: Deprecation of a stable components Nontrivial New components Architectural changes Do the easier thing if uncertain. (choosing a path is not final) Non-RFC process \u00b6 A (non-RFC) PR is likely to be asked to be reopend as a RFC if clearly not acceptable: Disproportionate breaking change (small inference breakage may be acceptable) Unsound Doesn't fit our general design philosophy around the problem Better as a crate Too marginal for std Significant implementation problems A PR may also be asked to be reopend as a RFC because an RFC is approriate. A (non-RFC) PR may be merged as unstable . In this case, the feature should have a fresh feature gate and an associated tracking issue for stabilisation. Docs are insta-stable and thus have no tracking issue. This may imply requiring a higher level of scrutiny for such changes. However, an accepted RFC is not a rubber-stamp for merging an implementation PR. Nor must an implementation PR perfectly match the RFC text. Implementation details may merit deviations, though obviously they should be justified. The RFC may be amended if deviations are substantial, but are not generally necessary. RFCs should favour immutability. The RFC + Issue + PR should form a total explanation of the current implementation. Once something has been merged as unstable, a shepherd should be assigned to promote and obtain feedback on the design. Every time a release cycle ends, the libs teams assesses the current unstable APIs and selects some number of them for potential stabilization during the next cycle. These are announced for FCP at the beginning of the cycle, and (possibly) stabilized just before the beta is cut. After the final comment period, an API should ideally take one of two paths: Stabilize if the change is desired, and consensus is reached Deprecate is the change is undesired, and consensus is reached Extend the FCP is the change cannot meet consensus If consensus still can't be reached, consider requiring a new RFC or just deprecating as \"too controversial for std\". If any problems are found with a newly stabilized API during its beta period, strongly favour reverting stability in order to prevent stabilizing a bad API. Due to the speed of the trains, this is not a serious delay (~2-3 months if it's not a major problem).","title":"Library Changes"},{"location":"libs_changes/#rfc-guidelines-libraries-sub-team","text":"","title":"RFC guidelines - libraries sub-team"},{"location":"libs_changes/#motivation","text":"RFCs are heavyweight: RFCs generally take at minimum 2 weeks from posting to land. In practice it can be more on the order of months for particularly controversial changes. RFCs are a lot of effort to write; especially for non-native speakers or for members of the community whose strengths are more technical than literary. RFCs may involve pre-RFCs and several rewrites to accommodate feedback. RFCs require a dedicated shepherd to herd the community and author towards consensus. RFCs require review from a majority of the subteam, as well as an official vote. RFCs can't be downgraded based on their complexity. Full process always applies. Easy RFCs may certainly land faster, though. RFCs can be very abstract and hard to grok the consequences of (no implementation). PRs are low overhead but potentially expensive nonetheless: Easy PRs can get insta-merged by any tremor libraries sub-team contributor. Harder PRs can be easily escalated. You can ping subject-matter experts for second opinions. Ping the whole team! Easier to grok the full consequences. Lots of tests and Crater to save the day. PRs can be accepted optimistically with bors, buildbot, and the trains to guard us from major mistakes making it into stable. The size of the nightly community at this point in time can still mean major community breakage regardless of trains, however. HOWEVER: Big PRs can be a lot of work to make only to have that work rejected for details that could have been hashed out first. RFCs are only meaningful if a significant and diverse portion of the community actively participates in them. The official teams are not sufficiently diverse to establish meaningful community consensus by agreeing amongst themselves. If there are tons of RFCs -- especially trivial ones -- people are less likely to engage with them. Official team members are super busy. Domain experts and industry professionals are super busy and have no responsibility to engage in RFCs. Since these are exactly the most important people to get involved in the RFC process, it is important that we be maximally friendly towards their needs.","title":"Motivation"},{"location":"libs_changes/#is-an-rfc-required","text":"The overarching philosophy is: do whatever is easiest . If an RFC would be less work than an implementation, that's a good sign that an RFC is necessary. That said, if you anticipate controversy, you might want to short-circuit straight to an RFC. Submit a PR if the change is a: Bugfix Docfix Perffixes (with no further impact) Submit an RFC if the change is a: Deprecation of a stable components Nontrivial New components Architectural changes Do the easier thing if uncertain. (choosing a path is not final)","title":"Is an RFC required?"},{"location":"libs_changes/#non-rfc-process","text":"A (non-RFC) PR is likely to be asked to be reopend as a RFC if clearly not acceptable: Disproportionate breaking change (small inference breakage may be acceptable) Unsound Doesn't fit our general design philosophy around the problem Better as a crate Too marginal for std Significant implementation problems A PR may also be asked to be reopend as a RFC because an RFC is approriate. A (non-RFC) PR may be merged as unstable . In this case, the feature should have a fresh feature gate and an associated tracking issue for stabilisation. Docs are insta-stable and thus have no tracking issue. This may imply requiring a higher level of scrutiny for such changes. However, an accepted RFC is not a rubber-stamp for merging an implementation PR. Nor must an implementation PR perfectly match the RFC text. Implementation details may merit deviations, though obviously they should be justified. The RFC may be amended if deviations are substantial, but are not generally necessary. RFCs should favour immutability. The RFC + Issue + PR should form a total explanation of the current implementation. Once something has been merged as unstable, a shepherd should be assigned to promote and obtain feedback on the design. Every time a release cycle ends, the libs teams assesses the current unstable APIs and selects some number of them for potential stabilization during the next cycle. These are announced for FCP at the beginning of the cycle, and (possibly) stabilized just before the beta is cut. After the final comment period, an API should ideally take one of two paths: Stabilize if the change is desired, and consensus is reached Deprecate is the change is undesired, and consensus is reached Extend the FCP is the change cannot meet consensus If consensus still can't be reached, consider requiring a new RFC or just deprecating as \"too controversial for std\". If any problems are found with a newly stabilized API during its beta period, strongly favour reverting stability in order to prevent stabilizing a bad API. Due to the speed of the trains, this is not a serious delay (~2-3 months if it's not a major problem).","title":"Non-RFC process"}]}